# Contradiction #6: Simulated Data Training
## ÏπòÎ™ÖÏ†Å Î∞úÍ≤¨: Exit Models Trained on WRONG Distribution

**Date**: 2025-10-15
**Severity**: üî¥ CRITICAL
**Discovery**: User identified in one word: "Simulated data?"

---

## Problem Statement

**Exit Models are trained on SIMULATED TRADES, not REAL ENTRY MODEL outputs.**

This creates a **fundamental distribution mismatch** between training and production.

---

## Evidence

### Training Code Analysis

**File**: `scripts/experiments/train_exit_models.py`

```python
# Line 136-138: Generate training data
for entry_idx in range(0, len(df) - 60, sample_every=5):
    # ‚ö†Ô∏è PROBLEM: Simulate trade from RANDOM entry points
    trade = simulate_trade_forward(entry_idx, df, direction, max_candles=48)

    # This generates entries EVERY 5-10 candles
    # Without considering Entry model predictions!
```

```python
# Line 54-107: simulate_trade_forward
def simulate_trade_forward(entry_idx, df, direction="LONG", max_candles=48):
    """Simulate a trade forward from entry point"""
    entry_price = df['close'].iloc[entry_idx]  # ‚Üê ANY candle!

    # No check if Entry model would actually signal here!
    # No filtering by Entry model probability!
```

### Training Statistics

**LONG Exit Model**:
```
Simulated trades: ~1,933 trades
Source: Random sampling every 5-10 candles
Entry rate: ~10-20% of all candles
Total samples: 88,739 exit decision points
```

**SHORT Exit Model**:
```
Simulated trades: ~1,936 trades
Source: Random sampling every 5-10 candles
Entry rate: ~10-20% of all candles
Total samples: 89,345 exit decision points
```

### Production Statistics

**Actual Entry Rate** (from V3 backtest):
```
Entry rate: 0.6% of candles (42.5 trades/week from 12*24*7 candles)
LONG signals: ~39 trades/week (prob_long >= 0.70)
SHORT signals: ~3.5 trades/week (prob_short >= 0.65)

Comparison:
Training: 10-20% entry rate (simulated)
Production: 0.6% entry rate (actual)
Mismatch: 17-33x DIFFERENCE!
```

---

## Mathematical Analysis

### Distribution Mismatch Proof

**Let D_train = distribution of simulated entries**
**Let D_prod = distribution of real Entry model entries**

```python
# Training distribution (simulated):
D_train ~ Uniform(all_candles, sample_every=5)
P(entry at candle_t | D_train) = 1/5 = 20%

# Production distribution (Entry model):
D_prod ~ Conditional(prob_entry >= threshold)
P(entry at candle_t | D_prod) = P(prob >= 0.70) ‚âà 0.6%

# Kullback-Leibler Divergence:
KL(D_prod || D_train) = VERY HIGH (distributions are completely different!)
```

### Feature Space Analysis

**Training Features** (at random entries):
```python
# Simulated entries see:
- All types of market conditions
- Low probability entries (prob < 0.50)
- High probability entries (prob > 0.70)
- Entries in unfavorable conditions
- Random technical indicator values

Distribution: Wide, covers all feature space
```

**Production Features** (at Entry model >= 0.70):
```python
# Real entries only when:
- Entry model learned specific patterns
- Probability >= 0.70 (top 1% of predictions)
- Favorable technical indicators
- Specific feature combinations that Entry model trained on

Distribution: Narrow, concentrated in high-probability regions
```

**Result**: Exit model trained on feature distribution X, but used on feature distribution Y (where X ‚â† Y)!

---

## Failure Modes

### Mode 1: Exit Model Sees Unfamiliar Patterns

```
Training: "I learned to exit from ANY entry (including bad ones)"
Production: "Wait, all these entries are HIGH QUALITY? I never saw this!"

Result: Exit model doesn't know how to handle consistently good entries
        It was trained on mix of good AND bad entries
```

### Mode 2: Overfitting to Simulation Artifacts

```
Training: "Peak P&L calculation from simulation (no slippage, no costs)"
Labels: "Exit at 80% of simulated peak"

Production: "Real peak is DIFFERENT due to slippage and costs!"
Labels: "Exit at 80% of wrong peak = suboptimal exit!"
```

### Mode 3: Entry Model Correlation Ignored

```
Training: "Exits sampled independently of Entry model"
Production: "All exits come from entries that Entry model selected"

Entry model may have biases:
- Prefers certain market regimes
- Prefers certain technical patterns
- Prefers certain times of day

Exit model NEVER LEARNED these biases!
```

---

## Quantitative Impact Estimate

### Training Data Quality

**Simulated LONG trades** (1,933 trades):
```yaml
Distribution:
  Random entries: 100% (every 5-10 candles)
  Reflects Entry model: 0% (no Entry model used)

Quality:
  Good entries (prob >= 0.70): ~20% (random chance)
  Bad entries (prob < 0.70): ~80%

Bias: Model learned to exit from MOSTLY BAD entries!
```

**Real LONG trades** (42.5/week):
```yaml
Distribution:
  Entry model filtered: 100% (prob >= 0.70)
  Random entries: 0%

Quality:
  Good entries: 100% (by definition, prob >= 0.70)
  Bad entries: 0%

Bias: All entries are HIGH QUALITY!
```

**Mismatch Score**: 80% of training data is IRRELEVANT to production!

### Expected Performance Degradation

```python
# Hypothesis: Exit model performance degrades when applied to Entry model outputs

Estimated Impact:
- Exit model accuracy: 89% (on test set of SIMULATED entries)
- Exit model accuracy: ??? (on REAL Entry model entries)

Pessimistic: 50-60% (complete distribution mismatch)
Realistic: 70-75% (partial transfer learning)
Optimistic: 85-87% (some generalization)

Currently claimed: 94.7% win rate
Likely reality: Much lower (unvalidated on real entries!)
```

---

## Root Cause Analysis

### Why Was This Approach Used?

**Practical Reason**: No real trading data available
```
Problem: Need labeled exit data to train Exit model
Solution (chosen): Simulate trades to generate labels
Alternative (correct): Collect real trading data first
```

**Theoretical Assumption**: Exit timing is independent of Entry quality
```
Assumption: "Exit decision only depends on current position state"
           "Doesn't matter HOW we entered, only WHERE we are now"

Reality: FALSE! Entry quality affects:
         - Initial momentum
         - Risk/reward ratio
         - Market regime at entry
         - Feature patterns at entry
```

### Correct Approach (Not Used)

```python
# Step 1: Collect real trading data
real_trades = []
for candle in historical_data:
    entry_signal = entry_model.predict(candle)
    if entry_signal >= threshold:
        # ACTUALLY ENTER (in backtest)
        real_trades.append(simulate_from_this_entry)

# Step 2: Label exit points from REAL entry model trades
for trade in real_trades:
    for candle in trade.candles:
        label = should_exit(candle, trade)
        training_data.append((candle.features, label))

# Step 3: Train Exit model on REAL entry model distribution
exit_model.train(training_data)
```

**Key Difference**: Only simulate trades that Entry model would ACTUALLY take!

---

## Solutions

### Solution 1: Retrain Exit Models with Entry Model Filtering (BEST)

**Approach**: Re-generate training data using Entry model predictions

```python
# Modified train_exit_models.py

def generate_exit_training_data_v2(df, entry_model, entry_scaler, threshold, direction="LONG"):
    """
    Generate training data using REAL Entry model predictions
    Only simulate trades where Entry model would actually signal!
    """
    samples = []
    trades_count = 0

    # Calculate Entry model predictions for ALL candles
    X = df[feature_columns].values
    X_scaled = entry_scaler.transform(X)
    entry_probs = entry_model.predict_proba(X_scaled)[:, 1]

    # Only simulate trades where Entry model signals
    for entry_idx in range(len(df) - 60):
        entry_prob = entry_probs[entry_idx]

        # ‚úÖ CRITICAL: Only enter if Entry model would signal!
        if entry_prob < threshold:
            continue  # Skip this entry (Entry model wouldn't take it)

        # Now simulate trade (only for REAL entry points)
        trade = simulate_trade_forward(entry_idx, df, direction, max_candles=48)

        # Rest of labeling logic...
```

**Expected Impact**:
```
Before: 1,933 LONG trades (random entries)
After: ~300-400 LONG trades (only Entry model >= 0.70)

Training data quality: 80% irrelevant ‚Üí 0% irrelevant
Distribution match: 0% ‚Üí 100%
Expected accuracy boost: +5-10% in production
```

**Implementation**: 2-3 days (retrain, validate, deploy)

### Solution 2: Ensemble Exit Model

**Approach**: Train multiple Exit models on different entry scenarios

```python
# Train 3 Exit models:
exit_model_low_prob = train_on_entries(prob_range=[0.50, 0.60])  # Low quality
exit_model_mid_prob = train_on_entries(prob_range=[0.60, 0.70])  # Medium quality
exit_model_high_prob = train_on_entries(prob_range=[0.70, 1.00]) # High quality (PRODUCTION!)

# Production usage:
if entry_prob >= 0.70:
    exit_decision = exit_model_high_prob.predict(features)  # Use appropriate model!
elif entry_prob >= 0.60:
    exit_decision = exit_model_mid_prob.predict(features)
else:
    exit_decision = exit_model_low_prob.predict(features)
```

**Expected Impact**:
```
Specialization: Each model optimized for specific entry quality
Accuracy: +3-7% (better than single model)
Complexity: Medium (manage 3 models)
```

**Implementation**: 3-5 days (train 3 models, validation)

### Solution 3: End-to-End RL (Most Fundamental)

**Approach**: Train Entry and Exit jointly with Reinforcement Learning

```python
# RL Agent learns:
# - When to enter (replaces Entry model)
# - When to exit (replaces Exit model)
# - How much to size (replaces Position Sizing)

# Single objective: Maximize cumulative P&L
# No distribution mismatch (entry and exit learned together!)

from stable_baselines3 import PPO

agent = PPO('MlpPolicy', env=TradingEnv(data))
agent.learn(total_timesteps=1_000_000)

# Production:
state = get_market_features()
action = agent.predict(state)  # [enter_long, enter_short, exit, hold]
```

**Expected Impact**:
```
Distribution mismatch: Eliminated (joint training)
Performance: Potentially +10-20% (optimal policy)
Complexity: Very high (requires RL expertise)
Risk: High (unproven in this domain)
```

**Implementation**: 2-3 weeks (research, experiment, validate)

### Solution 4: Transfer Learning Fine-Tuning (Quick Fix)

**Approach**: Fine-tune Exit model on small real dataset

```python
# Step 1: Collect 100-200 real trades from production
real_trades = collect_from_production(days=30)  # 30 days ‚âà 180 trades

# Step 2: Label exits from real trades
real_exit_data = label_exits(real_trades)  # ~3,000-5,000 samples

# Step 3: Fine-tune existing Exit model
exit_model.load('xgboost_v4_long_exit.pkl')
exit_model.fit(
    real_exit_data,
    learning_rate=0.01,  # Lower LR for fine-tuning
    n_estimators=50,  # Additional trees
    xgb_model=exit_model  # Warm start from existing model
)
```

**Expected Impact**:
```
Quick: Only 30 days of production data needed
Accuracy: +2-5% (adaptation to real distribution)
Risk: Low (small changes to existing model)
```

**Implementation**: 1-2 days (after 30 days of data collection)

---

## Immediate Actions

### 1. Validate Current Exit Model on Entry Model Outputs

**Script**:
```python
# Test Exit model on ONLY Entry model entries
def validate_exit_on_real_entries(df, entry_model, exit_model, entry_thresh=0.70):
    # Get Entry model predictions
    entry_probs = entry_model.predict_proba(...)[:, 1]

    # Filter to only Entry model entries
    real_entry_indices = np.where(entry_probs >= entry_thresh)[0]

    print(f"Total candles: {len(df)}")
    print(f"Simulated entries (training): {len(df) / 5}")  # Every 5 candles
    print(f"Real entries (Entry model): {len(real_entry_indices)}")
    print(f"Overlap: {len(real_entry_indices) / (len(df)/5) * 100:.1f}%")

    # Simulate trades from ONLY real entries
    for idx in real_entry_indices:
        trade = simulate_trade(idx, df)
        exit_accuracy = evaluate_exit_model(exit_model, trade)

    return exit_accuracy
```

**Run This First**: Understand actual performance degradation

### 2. Calculate Feature Distribution Divergence

```python
# Compare training vs production feature distributions
from scipy.stats import wasserstein_distance

train_features = simulated_entries_features  # From training
prod_features = entry_model_entries_features  # From real entries

for feat in feature_columns:
    divergence = wasserstein_distance(train_features[feat], prod_features[feat])
    if divergence > threshold:
        print(f"‚ö†Ô∏è Feature {feat} has high divergence: {divergence:.4f}")
```

### 3. Emergency Fallback: Disable Exit Model

**If Exit model performs poorly**:
```python
# Revert to rule-based exits
# These are KNOWN to work (backtest validated)

USE_ML_EXIT = False  # Emergency flag

if USE_ML_EXIT:
    exit_decision = exit_model.predict(features)
else:
    # Rule-based (proven in backtest)
    if pnl_pct <= -0.01:
        exit_decision = True  # Stop loss
    elif pnl_pct >= 0.02:
        exit_decision = True  # Take profit
    elif hours_held >= 4:
        exit_decision = True  # Max hold
```

---

## Conclusion

**Simulated Data Training is a FUNDAMENTAL FLAW** in the current system.

**Impact**:
- Exit model trained on WRONG distribution (random entries)
- Production uses DIFFERENT distribution (Entry model >= 0.70)
- 80% of training data is irrelevant to production
- Likely performance degradation: 5-20% (unquantified)

**Priority**: üî¥ CRITICAL (along with Threshold Optimization)

**Recommended Solution**: Solution 1 (Retrain with Entry Model Filtering)
- Timeline: 2-3 days
- Expected improvement: +5-10% accuracy
- Risk: Low (straightforward fix)

**Alternative**: Solution 4 (Fine-tuning on real data)
- Timeline: 30 days data collection + 1-2 days training
- Expected improvement: +2-5% accuracy
- Risk: Very low

---

**User's Question**: "Simulated data?" ‚Üí **Identified the 6th critical contradiction**

**Status**: üî¥ UNRESOLVED - Requires immediate attention
**Next Action**: Validate current Exit model performance on real Entry model outputs
