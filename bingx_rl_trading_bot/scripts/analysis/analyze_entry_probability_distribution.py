"""
Analyze Entry Probability Distribution - Retrained Models
==========================================================

Purpose:
  Understand why trade frequency is so low (0.43/day)
  Analyze probability distribution generated by retrained models
  Determine optimal threshold for desired trade frequency

Analysis:
  1. Load retrained LONG/SHORT entry models (20251029_081454)
  2. Generate predictions for entire 14-day holdout period
  3. Analyze probability distribution (percentiles, histogram)
  4. Determine what threshold achieves 2.0+ trades/day

Created: 2025-10-29
"""

import sys
from pathlib import Path
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import pandas as pd
import numpy as np
import joblib
import matplotlib.pyplot as plt

# Configuration
HOLDOUT_DAYS = 14
DATA_DIR = PROJECT_ROOT / "data" / "features"
MODELS_DIR = PROJECT_ROOT / "models"

print("="*80)
print("ENTRY PROBABILITY DISTRIBUTION ANALYSIS")
print("="*80)
print()
print("Models: RETRAINED (timestamp: 20251029_081454)")
print("Period: 14-day Holdout (Oct 13-26)")
print()

# Load data
print("-"*80)
print("Loading Data")
print("-"*80)

df = pd.read_csv(DATA_DIR / "BTCUSDT_5m_features.csv")
print(f"✅ Loaded {len(df):,} candles")

# Select holdout period
holdout_candles = HOLDOUT_DAYS * 24 * 12
holdout_start_idx = len(df) - holdout_candles
df = df.iloc[holdout_start_idx:].copy().reset_index(drop=True)

print(f"✅ Holdout period: {len(df):,} candles ({HOLDOUT_DAYS} days)")
print(f"   {df['timestamp'].iloc[0]} to {df['timestamp'].iloc[-1]}")
print()

# Load models
print("-"*80)
print("Loading Retrained Entry Models")
print("-"*80)

# LONG Entry
long_entry_model = joblib.load(MODELS_DIR / "xgboost_long_entry_retrained_latest_20251029_081454.pkl")
long_entry_scaler = joblib.load(MODELS_DIR / "xgboost_long_entry_retrained_latest_20251029_081454_scaler.pkl")
with open(MODELS_DIR / "xgboost_long_entry_retrained_latest_20251029_081454_features.txt", 'r') as f:
    long_entry_features = [line.strip() for line in f if line.strip()]

# SHORT Entry
short_entry_model = joblib.load(MODELS_DIR / "xgboost_short_entry_retrained_latest_20251029_081454.pkl")
short_entry_scaler = joblib.load(MODELS_DIR / "xgboost_short_entry_retrained_latest_20251029_081454_scaler.pkl")
with open(MODELS_DIR / "xgboost_short_entry_retrained_latest_20251029_081454_features.txt", 'r') as f:
    short_entry_features = [line.strip() for line in f if line.strip()]

print(f"✅ LONG Entry: {len(long_entry_features)} features")
print(f"✅ SHORT Entry: {len(short_entry_features)} features")
print()

# Generate predictions for all candles
print("-"*80)
print("Generating Probability Predictions")
print("-"*80)

long_probs = []
short_probs = []

for i in range(len(df)):
    row = df.iloc[i]

    # LONG prediction
    try:
        X_long = row[long_entry_features].values.reshape(1, -1)
        X_long_scaled = long_entry_scaler.transform(X_long)
        long_prob = long_entry_model.predict_proba(X_long_scaled)[0][1]
        long_probs.append(long_prob)
    except:
        long_probs.append(np.nan)

    # SHORT prediction
    try:
        X_short = row[short_entry_features].values.reshape(1, -1)
        X_short_scaled = short_entry_scaler.transform(X_short)
        short_prob = short_entry_model.predict_proba(X_short_scaled)[0][1]
        short_probs.append(short_prob)
    except:
        short_probs.append(np.nan)

df['long_prob'] = long_probs
df['short_prob'] = short_probs

# Remove NaN values
df_valid = df.dropna(subset=['long_prob', 'short_prob']).copy()

print(f"✅ Generated predictions for {len(df_valid):,} candles")
print()

# Analyze LONG probability distribution
print("="*80)
print("LONG ENTRY PROBABILITY DISTRIBUTION")
print("="*80)
print()

long_stats = df_valid['long_prob'].describe(percentiles=[.25, .5, .75, .90, .95, .99])
print("Statistics:")
for stat, value in long_stats.items():
    print(f"  {stat}: {value:.4f}")
print()

# Count by threshold
thresholds = [0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90]
print("Samples >= Threshold:")
for thresh in thresholds:
    count = (df_valid['long_prob'] >= thresh).sum()
    pct = count / len(df_valid) * 100
    trades_per_day = count / HOLDOUT_DAYS
    print(f"  ≥{thresh:.2f}: {count:,} samples ({pct:.2f}%) → {trades_per_day:.2f} trades/day")
print()

# Analyze SHORT probability distribution
print("="*80)
print("SHORT ENTRY PROBABILITY DISTRIBUTION")
print("="*80)
print()

short_stats = df_valid['short_prob'].describe(percentiles=[.25, .5, .75, .90, .95, .99])
print("Statistics:")
for stat, value in short_stats.items():
    print(f"  {stat}: {value:.4f}")
print()

# Count by threshold
print("Samples >= Threshold:")
for thresh in thresholds:
    count = (df_valid['short_prob'] >= thresh).sum()
    pct = count / len(df_valid) * 100
    trades_per_day = count / HOLDOUT_DAYS
    print(f"  ≥{thresh:.2f}: {count:,} samples ({pct:.2f}%) → {trades_per_day:.2f} trades/day")
print()

# Combined analysis (max of LONG/SHORT)
print("="*80)
print("COMBINED ENTRY OPPORTUNITIES (max of LONG/SHORT)")
print("="*80)
print()

df_valid['max_prob'] = df_valid[['long_prob', 'short_prob']].max(axis=1)

print("Samples >= Threshold (either LONG or SHORT):")
for thresh in thresholds:
    count = (df_valid['max_prob'] >= thresh).sum()
    pct = count / len(df_valid) * 100
    trades_per_day = count / HOLDOUT_DAYS
    status = "✅" if trades_per_day >= 2.0 else "❌"
    print(f"  ≥{thresh:.2f}: {count:,} samples ({pct:.2f}%) → {trades_per_day:.2f} trades/day {status}")
print()

# Find optimal threshold for 2.0+ trades/day
print("="*80)
print("OPTIMAL THRESHOLD RECOMMENDATION")
print("="*80)
print()

target_trades_per_day = 2.0
target_samples = int(target_trades_per_day * HOLDOUT_DAYS)

print(f"Target: {target_trades_per_day} trades/day = {target_samples} total trades in {HOLDOUT_DAYS} days")
print()

# Binary search for optimal threshold
for thresh in np.arange(0.30, 0.90, 0.01):
    count = (df_valid['max_prob'] >= thresh).sum()
    trades_per_day = count / HOLDOUT_DAYS

    if trades_per_day >= target_trades_per_day:
        optimal_threshold = thresh
        optimal_count = count
        optimal_freq = trades_per_day
        break

print(f"Optimal Threshold: {optimal_threshold:.2f}")
print(f"  Expected trades: {optimal_count} ({optimal_freq:.2f}/day)")
print(f"  Percentile: {(df_valid['max_prob'] < optimal_threshold).sum() / len(df_valid) * 100:.1f}%")
print()

# Distribution breakdown at optimal threshold
long_count = (df_valid['long_prob'] >= optimal_threshold).sum()
short_count = (df_valid['short_prob'] >= optimal_threshold).sum()
print(f"Breakdown at threshold {optimal_threshold:.2f}:")
print(f"  LONG: {long_count} ({long_count/optimal_count*100:.1f}%)")
print(f"  SHORT: {short_count} ({short_count/optimal_count*100:.1f}%)")
print()

print("="*80)
print("SUMMARY")
print("="*80)
print()
print(f"Current Threshold (0.75):")
print(f"  LONG: {(df_valid['long_prob'] >= 0.75).sum()} samples")
print(f"  SHORT: {(df_valid['short_prob'] >= 0.75).sum()} samples")
print(f"  Combined: {(df_valid['max_prob'] >= 0.75).sum()} samples → {(df_valid['max_prob'] >= 0.75).sum()/HOLDOUT_DAYS:.2f} trades/day ❌")
print()
print(f"Recommended Threshold ({optimal_threshold:.2f}):")
print(f"  LONG: {long_count} samples")
print(f"  SHORT: {short_count} samples")
print(f"  Combined: {optimal_count} samples → {optimal_freq:.2f} trades/day ✅")
print()
print(f"Change Required: 0.75 → {optimal_threshold:.2f} ({(optimal_threshold-0.75):.2f})")
print()
