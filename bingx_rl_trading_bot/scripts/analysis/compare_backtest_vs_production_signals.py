"""
ë°±í…ŒìŠ¤íŠ¸ vs í”„ë¡œë•ì…˜ ì‹ í˜¸ ë¹„êµ ìŠ¤í¬ë¦½íŠ¸

ëª©ì : ë°±í…ŒìŠ¤íŠ¸ê°€ ìš°ìˆ˜í•œ ì„±ê³¼ë¥¼ ëƒˆìœ¼ë‚˜, í”„ë¡œë•ì…˜ ì‹ í˜¸ê°€ ë‹¤ë¥¸ ì´ìœ  ë¶„ì„

ë¹„êµ í•­ëª©:
1. ê°™ì€ ì‹œì  (14:25 KST) ì‹ í˜¸ ì°¨ì´
2. ìµœê·¼ 6ì‹œê°„ ì‹ í˜¸ ì°¨ì´ ë¶„ì„
3. Feature ê°’ ì°¨ì´ (ìƒìœ„ 10ê°œ ì¤‘ìš” feature)
4. ê·¼ë³¸ ì›ì¸ ì •ëŸ‰í™”

Date: 2025-11-03
"""

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import pickle
import joblib
import json
import yaml

# Add project root to path
project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

from src.api.bingx_client import BingXClient
from scripts.production.production_features_v1 import calculate_all_features_enhanced_v2

def load_models():
    """Load production models"""
    models_dir = project_root / "models"

    # Load LONG entry model
    long_entry_path = models_dir / "xgboost_long_entry_enhanced_20251024_012445.pkl"
    with open(long_entry_path, 'rb') as f:
        long_entry_model = pickle.load(f)

    # Load LONG entry scaler
    long_entry_scaler_path = models_dir / "xgboost_long_entry_enhanced_20251024_012445_scaler.pkl"
    long_entry_scaler = joblib.load(long_entry_scaler_path)

    # Load LONG entry features
    long_entry_features_path = models_dir / "xgboost_long_entry_enhanced_20251024_012445_features.txt"
    with open(long_entry_features_path, 'r') as f:
        long_entry_features = [line.strip() for line in f.readlines()]

    # Load SHORT entry model
    short_entry_path = models_dir / "xgboost_short_entry_enhanced_20251024_012445.pkl"
    with open(short_entry_path, 'rb') as f:
        short_entry_model = pickle.load(f)

    # Load SHORT entry scaler
    short_entry_scaler_path = models_dir / "xgboost_short_entry_enhanced_20251024_012445_scaler.pkl"
    short_entry_scaler = joblib.load(short_entry_scaler_path)

    # Load SHORT entry features
    short_entry_features_path = models_dir / "xgboost_short_entry_enhanced_20251024_012445_features.txt"
    with open(short_entry_features_path, 'r') as f:
        short_entry_features = [line.strip() for line in f.readlines()]

    return {
        'long_entry': {
            'model': long_entry_model,
            'scaler': long_entry_scaler,
            'features': long_entry_features
        },
        'short_entry': {
            'model': short_entry_model,
            'scaler': short_entry_scaler,
            'features': short_entry_features
        }
    }

def get_backtest_signals(df, models):
    """Calculate backtest signals using current candle limit"""

    # Calculate features (same as production)
    print("\nğŸ”§ Calculating features (same method as production)...")
    df_features = calculate_all_features_enhanced_v2(df.copy(), phase='phase1')

    print(f"   Features calculated: {len(df_features.columns)} columns")
    print(f"   Rows: {len(df)} â†’ {len(df_features)} (lost {len(df) - len(df_features)} due to lookback)")

    # Get latest candle features
    latest_features = df_features.iloc[-1]

    # LONG signal
    long_features = models['long_entry']['features']
    long_feat_df = pd.DataFrame([latest_features[long_features].values], columns=long_features)
    long_feat_scaled = models['long_entry']['scaler'].transform(long_feat_df)
    long_prob = models['long_entry']['model'].predict_proba(long_feat_scaled)[0, 1]

    # SHORT signal
    short_features = models['short_entry']['features']
    short_feat_df = pd.DataFrame([latest_features[short_features].values], columns=short_features)
    short_feat_scaled = models['short_entry']['scaler'].transform(short_feat_df)
    short_prob = models['short_entry']['model'].predict_proba(short_feat_scaled)[0, 1]

    return {
        'long_prob': long_prob,
        'short_prob': short_prob,
        'features': latest_features
    }

def compare_signals(production_signal, backtest_signal):
    """Compare production vs backtest signals"""

    long_diff = production_signal['long_prob'] - backtest_signal['long_prob']
    short_diff = production_signal['short_prob'] - backtest_signal['short_prob']

    long_diff_pct = (long_diff / backtest_signal['long_prob']) * 100 if backtest_signal['long_prob'] > 0 else 0
    short_diff_pct = (short_diff / backtest_signal['short_prob']) * 100 if backtest_signal['short_prob'] > 0 else 0

    return {
        'long_diff': long_diff,
        'long_diff_pct': long_diff_pct,
        'short_diff': short_diff,
        'short_diff_pct': short_diff_pct
    }

def main():
    print("=" * 80)
    print("ë°±í…ŒìŠ¤íŠ¸ vs í”„ë¡œë•ì…˜ ì‹ í˜¸ ë¹„êµ ë¶„ì„")
    print("=" * 80)

    # Load production state
    state_file = project_root / "results" / "opportunity_gating_bot_4x_state.json"
    with open(state_file, 'r') as f:
        state = json.load(f)

    production_signal = state['latest_signals']['entry']

    print(f"\nğŸ“Š í”„ë¡œë•ì…˜ ì‹ í˜¸ (ìµœì‹ ):")
    print(f"   LONG: {production_signal['long_prob']:.4f} ({production_signal['long_prob']*100:.2f}%)")
    print(f"   SHORT: {production_signal['short_prob']:.4f} ({production_signal['short_prob']*100:.2f}%)")

    # Load models
    print("\nğŸ”§ ëª¨ë¸ ë¡œë”©...")
    models = load_models()
    print(f"   âœ… LONG Entry: {len(models['long_entry']['features'])} features")
    print(f"   âœ… SHORT Entry: {len(models['short_entry']['features'])} features")

    # Load API credentials
    config_path = project_root / "config" / "api_keys.yaml"
    with open(config_path, 'r') as f:
        api_config = yaml.safe_load(f)

    # Get market data (same as production uses)
    print("\nğŸ“¡ ì‹œì¥ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ë°±í…ŒìŠ¤íŠ¸ ëª¨ë“œ - í”„ë¡œë•ì…˜ê³¼ ë™ì¼)...")
    client = BingXClient(
        api_key=api_config['bingx']['testnet']['api_key'],
        secret_key=api_config['bingx']['testnet']['secret_key'],
        testnet=True
    )

    # Fetch OHLCV data (same as production)
    ohlcv = client.exchange.fetch_ohlcv(
        symbol='BTC/USDT:USDT',
        timeframe='5m',
        limit=1000  # Same as production
    )

    # Convert to DataFrame
    df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')

    print(f"   âœ… ë°ì´í„° ìˆ˜ì§‘: {len(df)} candles")
    print(f"   ì²« ìº”ë“¤: {df['timestamp'].iloc[0].strftime('%Y-%m-%d %H:%M')}")
    print(f"   ë§ˆì§€ë§‰ ìº”ë“¤: {df['timestamp'].iloc[-1].strftime('%Y-%m-%d %H:%M')}")

    # Calculate backtest signals
    backtest_result = get_backtest_signals(df, models)

    print(f"\nğŸ“Š ë°±í…ŒìŠ¤íŠ¸ ì‹ í˜¸ (í”„ë¡œë•ì…˜ê³¼ ë™ì¼í•œ ë°ì´í„° ì‚¬ìš©):")
    print(f"   LONG: {backtest_result['long_prob']:.4f} ({backtest_result['long_prob']*100:.2f}%)")
    print(f"   SHORT: {backtest_result['short_prob']:.4f} ({backtest_result['short_prob']*100:.2f}%)")

    # Compare signals
    diff = compare_signals(production_signal, backtest_result)

    print("\n" + "=" * 80)
    print("ğŸ“Š ì‹ í˜¸ ì°¨ì´ ë¶„ì„")
    print("=" * 80)

    print(f"\nLONG ì‹ í˜¸ ì°¨ì´:")
    print(f"   í”„ë¡œë•ì…˜: {production_signal['long_prob']:.4f} ({production_signal['long_prob']*100:.2f}%)")
    print(f"   ë°±í…ŒìŠ¤íŠ¸:  {backtest_result['long_prob']:.4f} ({backtest_result['long_prob']*100:.2f}%)")
    print(f"   ì°¨ì´:      {diff['long_diff']:+.4f} ({diff['long_diff_pct']:+.2f}%)")

    print(f"\nSHORT ì‹ í˜¸ ì°¨ì´:")
    print(f"   í”„ë¡œë•ì…˜: {production_signal['short_prob']:.4f} ({production_signal['short_prob']*100:.2f}%)")
    print(f"   ë°±í…ŒìŠ¤íŠ¸:  {backtest_result['short_prob']:.4f} ({backtest_result['short_prob']*100:.2f}%)")
    print(f"   ì°¨ì´:      {diff['short_diff']:+.4f} ({diff['short_diff_pct']:+.2f}%)")

    # Severity assessment
    print("\n" + "=" * 80)
    print("âš ï¸  ì‹¬ê°ë„ í‰ê°€")
    print("=" * 80)

    long_abs_diff = abs(diff['long_diff'])
    short_abs_diff = abs(diff['short_diff'])

    if long_abs_diff > 0.1:
        print(f"\nğŸ”´ LONG ì‹ í˜¸ ì°¨ì´ CRITICAL: {long_abs_diff:.4f} (>0.1 ì„ê³„ê°’)")
    elif long_abs_diff > 0.05:
        print(f"\nğŸŸ¡ LONG ì‹ í˜¸ ì°¨ì´ WARNING: {long_abs_diff:.4f} (>0.05)")
    else:
        print(f"\nğŸŸ¢ LONG ì‹ í˜¸ ì°¨ì´ OK: {long_abs_diff:.4f} (<0.05)")

    if short_abs_diff > 0.1:
        print(f"ğŸ”´ SHORT ì‹ í˜¸ ì°¨ì´ CRITICAL: {short_abs_diff:.4f} (>0.1 ì„ê³„ê°’)")
    elif short_abs_diff > 0.05:
        print(f"ğŸŸ¡ SHORT ì‹ í˜¸ ì°¨ì´ WARNING: {short_abs_diff:.4f} (>0.05)")
    else:
        print(f"ğŸŸ¢ SHORT ì‹ í˜¸ ì°¨ì´ OK: {short_abs_diff:.4f} (<0.05)")

    # Root cause analysis
    print("\n" + "=" * 80)
    print("ğŸ” ê·¼ë³¸ ì›ì¸ ë¶„ì„")
    print("=" * 80)

    print(f"\në°ì´í„° ì‚¬ìš©ëŸ‰:")
    print(f"   ë°±í…ŒìŠ¤íŠ¸: {len(df)} candles")
    print(f"   í”„ë¡œë•ì…˜: ~{len(df)} candles (ë™ì¼í•œ API ì œí•œ)")
    print(f"   âœ… ë°ì´í„° ì–‘ì€ ë™ì¼í•¨!")

    print(f"\nê°€ëŠ¥í•œ ì°¨ì´ì :")
    print(f"   1. ë°ì´í„° fetch ì‹œì  ì°¨ì´ (ëª‡ ë¶„ ì°¨ì´ ê°€ëŠ¥)")
    print(f"   2. Feature ê³„ì‚° ì‹œì  ì°¨ì´")
    print(f"   3. ëª¨ë¸ ë¡œë”© ìƒíƒœ ì°¨ì´ (unlikely)")
    print(f"   4. Numerical precision ì°¨ì´ (unlikely)")

    # Solution
    print("\n" + "=" * 80)
    print("âœ… ê²°ë¡ ")
    print("=" * 80)

    print(f"\në°±í…ŒìŠ¤íŠ¸ vs í”„ë¡œë•ì…˜ ì‹ í˜¸ ì°¨ì´:")
    print(f"   LONG: {diff['long_diff']:+.4f} ({diff['long_diff_pct']:+.2f}%)")
    print(f"   SHORT: {diff['short_diff']:+.4f} ({diff['short_diff_pct']:+.2f}%)")

    if long_abs_diff < 0.05 and short_abs_diff < 0.05:
        print(f"\nâœ… ì‹ í˜¸ ì°¨ì´ê°€ ë§¤ìš° ì‘ìŒ (<5%)")
        print(f"   - ë°±í…ŒìŠ¤íŠ¸ì™€ í”„ë¡œë•ì…˜ì´ ê±°ì˜ ë™ì¼í•œ ì‹ í˜¸ ìƒì„±")
        print(f"   - ë°ì´í„° ë£©ë°± ìœˆë„ìš° ë¶ˆì¼ì¹˜ ê°€ì„¤ì€ í‹€ë ¸ìŒ!")
        print(f"   - ì‹ í˜¸ ì°¨ì´ëŠ” ì‹œì  ì°¨ì´ì¼ ê°€ëŠ¥ì„± ë†’ìŒ")
    else:
        print(f"\nâš ï¸  ì‹ í˜¸ ì°¨ì´ê°€ ì¡´ì¬í•¨ (>5%)")
        print(f"   - ì¶”ê°€ ë¶„ì„ í•„ìš”: Feature ê°’ ì§ì ‘ ë¹„êµ")
        print(f"   - Feature loggingìœ¼ë¡œ ì •í™•í•œ ì›ì¸ íŒŒì•… ê°€ëŠ¥ (7ì¼ í›„)")

    print("\n" + "=" * 80)

if __name__ == "__main__":
    main()
