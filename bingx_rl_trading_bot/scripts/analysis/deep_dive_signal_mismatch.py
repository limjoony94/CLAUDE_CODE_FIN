#!/usr/bin/env python3
"""
Deep Dive Signal Mismatch Analysis
===================================
Analyze WHY signals differ even when current candle data matches.
Focus on lookback window and feature calculation differences.
"""

import pandas as pd
import numpy as np
import sys
from pathlib import Path
import yaml

PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.api.bingx_client import BingXClient
from scripts.experiments.calculate_all_features_enhanced_v2 import calculate_all_features_enhanced_v2
from scripts.experiments.retrain_exit_models_opportunity_gating import prepare_exit_features
import pickle
import joblib

print("=" * 80)
print("DEEP DIVE: Signal Mismatch Analysis")
print("=" * 80)

# Focus on 15:05 and 15:10 (data matches but signals differ)
test_cases = [
    {'timestamp_utc': '2025-10-26 15:05:00', 'name': '15:05'},
    {'timestamp_utc': '2025-10-26 15:10:00', 'name': '15:10'},
]

# 1. Load both data sources
print("\n1. Loading data from both sources...")
CONFIG_FILE = PROJECT_ROOT / "config" / "api_keys.yaml"
with open(CONFIG_FILE, 'r') as f:
    config = yaml.safe_load(f)

client = BingXClient(
    api_key=config['bingx']['mainnet']['api_key'],
    secret_key=config['bingx']['mainnet']['secret_key'],
    testnet=False
)

# Fetch API data with enough lookback (1000+ candles)
start_time = pd.to_datetime("2025-10-23 04:00:00")
end_time = pd.to_datetime("2025-10-26 15:30:00")
start_ms = int(start_time.timestamp() * 1000)
end_ms = int(end_time.timestamp() * 1000)

print("   Fetching API data...")
api_data = client.get_klines(
    symbol="BTC-USDT",
    interval="5m",
    start_time=start_ms,
    end_time=end_ms,
    limit=1440
)

df_api = pd.DataFrame(api_data, columns=['time', 'open', 'high', 'low', 'close', 'volume'])
df_api['timestamp'] = pd.to_datetime(df_api['time'], unit='ms', utc=True).dt.tz_localize(None)
df_api[['open', 'high', 'low', 'close', 'volume']] = df_api[['open', 'high', 'low', 'close', 'volume']].astype(float)
df_api = df_api.drop('time', axis=1)

print(f"   ‚úÖ API: {len(df_api)} candles ({df_api['timestamp'].min()} ~ {df_api['timestamp'].max()})")

# Load CSV
CSV_FILE = PROJECT_ROOT / "data" / "historical" / "BTCUSDT_5m_max.csv"
df_csv = pd.read_csv(CSV_FILE)
df_csv['timestamp'] = pd.to_datetime(df_csv['timestamp'])
df_csv = df_csv[
    (df_csv['timestamp'] >= df_api['timestamp'].min()) &
    (df_csv['timestamp'] <= df_api['timestamp'].max())
].copy()

print(f"   ‚úÖ CSV: {len(df_csv)} candles ({df_csv['timestamp'].min()} ~ {df_csv['timestamp'].max()})")

# 2. Compare entire lookback window
print("\n2. Comparing entire lookback window...")
merged = pd.merge(
    df_api, df_csv,
    on='timestamp',
    suffixes=('_api', '_csv'),
    how='inner'
)

# Calculate differences
merged['close_diff'] = merged['close_api'] - merged['close_csv']
merged['high_diff'] = merged['high_api'] - merged['high_csv']
merged['low_diff'] = merged['low_api'] - merged['low_csv']
merged['volume_diff'] = merged['volume_api'] - merged['volume_csv']

# Statistics
print(f"\n   Matched candles: {len(merged)}")
print(f"\n   Close price differences:")
print(f"     Mean: {merged['close_diff'].mean():.4f}")
print(f"     Std:  {merged['close_diff'].std():.4f}")
print(f"     Max:  {merged['close_diff'].max():.4f}")
print(f"     Min:  {merged['close_diff'].min():.4f}")

# Count perfect matches
perfect_matches = (
    (merged['close_diff'].abs() < 0.01) &
    (merged['high_diff'].abs() < 0.01) &
    (merged['low_diff'].abs() < 0.01)
).sum()

print(f"\n   Perfect matches: {perfect_matches}/{len(merged)} ({perfect_matches/len(merged)*100:.1f}%)")

# Find mismatches
mismatches = merged[merged['close_diff'].abs() >= 0.01].copy()
print(f"   Mismatches (>0.01): {len(mismatches)}")

if len(mismatches) > 0:
    print(f"\n   üìã Top 10 mismatches:")
    print(f"   {'Timestamp':<20} {'API Close':>12} {'CSV Close':>12} {'Diff':>10}")
    print("   " + "-" * 60)
    for _, row in mismatches.nlargest(10, 'close_diff', keep='all').iterrows():
        print(f"   {str(row['timestamp']):<20} {row['close_api']:>12.2f} {row['close_csv']:>12.2f} {row['close_diff']:>+10.2f}")

# 3. Analyze lookback windows for test cases
print("\n" + "=" * 80)
print("3. Analyzing lookback windows for signal generation")
print("=" * 80)

for test in test_cases:
    ts = pd.to_datetime(test['timestamp_utc'])
    print(f"\nüìç {test['name']} ({ts})")
    print("-" * 80)

    # Get 1000-candle lookback window (same as production)
    idx_api = df_api[df_api['timestamp'] == ts].index[0]
    idx_csv = df_csv[df_csv['timestamp'] == ts].index[0]

    lookback_api = df_api.iloc[max(0, idx_api-1000):idx_api+1].copy()
    lookback_csv = df_csv.iloc[max(0, idx_csv-1000):idx_csv+1].copy()

    print(f"   API lookback: {len(lookback_api)} candles")
    print(f"   CSV lookback: {len(lookback_csv)} candles")

    # Compare lookback windows
    lookback_merged = pd.merge(
        lookback_api[['timestamp', 'close', 'high', 'low', 'volume']],
        lookback_csv[['timestamp', 'close', 'high', 'low', 'volume']],
        on='timestamp',
        suffixes=('_api', '_csv'),
        how='inner'
    )

    lookback_merged['close_diff'] = lookback_merged['close_api'] - lookback_merged['close_csv']

    # Count differences in lookback
    lookback_diffs = (lookback_merged['close_diff'].abs() >= 0.01).sum()

    print(f"\n   Lookback window analysis:")
    print(f"     Total candles in window: {len(lookback_merged)}")
    print(f"     Candles with differences: {lookback_diffs}")
    print(f"     Perfect matches: {len(lookback_merged) - lookback_diffs}")

    if lookback_diffs > 0:
        print(f"\n   ‚ö†Ô∏è Found {lookback_diffs} different candles in lookback window!")
        print(f"   This explains why signals differ even though current candle matches.")

        # Show which candles differ
        diff_candles = lookback_merged[lookback_merged['close_diff'].abs() >= 0.01]
        print(f"\n   Different candles in lookback:")
        print(f"   {'Timestamp':<20} {'API Close':>12} {'CSV Close':>12} {'Diff':>10}")
        print("   " + "-" * 60)
        for _, row in diff_candles.tail(10).iterrows():
            print(f"   {str(row['timestamp']):<20} {row['close_api']:>12.2f} {row['close_csv']:>12.2f} {row['close_diff']:>+10.2f}")

# 4. Feature comparison
print("\n" + "=" * 80)
print("4. Feature-level comparison")
print("=" * 80)

# Calculate features for both
print("\n   Calculating features from API data...")
df_api_features = calculate_all_features_enhanced_v2(df_api, phase='phase1')
df_api_features = prepare_exit_features(df_api_features)

print("\n   Calculating features from CSV data...")
df_csv_features = calculate_all_features_enhanced_v2(df_csv, phase='phase1')
df_csv_features = prepare_exit_features(df_csv_features)

# Load models
MODELS_DIR = PROJECT_ROOT / "models"
long_model = pickle.load(open(MODELS_DIR / "xgboost_long_entry_enhanced_20251024_012445.pkl", 'rb'))
long_scaler = joblib.load(MODELS_DIR / "xgboost_long_entry_enhanced_20251024_012445_scaler.pkl")
with open(MODELS_DIR / "xgboost_long_entry_enhanced_20251024_012445_features.txt", 'r') as f:
    long_features = [line.strip() for line in f.readlines()]

print(f"\n   Model expects {len(long_features)} features")

# Compare features for test timestamps
for test in test_cases:
    ts = pd.to_datetime(test['timestamp_utc'])
    print(f"\nüìç {test['name']} - Feature comparison")
    print("-" * 80)

    row_api = df_api_features[df_api_features['timestamp'] == ts]
    row_csv = df_csv_features[df_csv_features['timestamp'] == ts]

    if len(row_api) == 0 or len(row_csv) == 0:
        print(f"   ‚ö†Ô∏è Features not available (lost in feature calculation)")
        continue

    row_api = row_api.iloc[0]
    row_csv = row_csv.iloc[0]

    # Compare key features
    feature_diffs = []
    for feat in long_features[:20]:  # Check first 20 features
        if feat in df_api_features.columns and feat in df_csv_features.columns:
            api_val = row_api[feat]
            csv_val = row_csv[feat]
            diff = abs(api_val - csv_val)

            if diff > 1e-6:  # Significant difference
                feature_diffs.append({
                    'feature': feat,
                    'api': api_val,
                    'csv': csv_val,
                    'diff': diff,
                    'pct': (diff / abs(csv_val) * 100) if abs(csv_val) > 1e-10 else 0
                })

    if feature_diffs:
        print(f"\n   Found {len(feature_diffs)} features with differences:")
        print(f"   {'Feature':<30} {'API':>12} {'CSV':>12} {'Diff':>12} {'Diff %':>10}")
        print("   " + "-" * 80)
        for fd in sorted(feature_diffs, key=lambda x: x['diff'], reverse=True)[:10]:
            print(f"   {fd['feature']:<30} {fd['api']:>12.6f} {fd['csv']:>12.6f} {fd['diff']:>12.6f} {fd['pct']:>9.2f}%")
    else:
        print(f"   ‚úÖ All features match (within numerical precision)")

# 5. Summary
print("\n" + "=" * 80)
print("ANALYSIS SUMMARY")
print("=" * 80)

print(f"""
üîç Key Findings:

1. Current Candle Data:
   - 15:05 and 15:10: 100% identical OHLCV data ‚úÖ
   - 15:25: Different (CSV captured incomplete candle) ‚ùå

2. Lookback Window:
   - Need to check if ANY historical candles differ
   - Even 1 different candle in 1000-candle window ‚Üí different features

3. Feature Propagation:
   - Technical indicators use historical data
   - Small data differences ‚Üí cascade to features ‚Üí affect signals

4. Signal Sensitivity:
   - LONG model: More sensitive to feature changes
   - SHORT model: More stable (4/5 matches vs LONG's 2/5)

‚úÖ CONCLUSION:
   Signal differences are EXPECTED when:
   - Lookback window contains ANY different candles
   - Features depend on historical data (MA, RSI, etc.)
   - Model is sensitive to feature variations

   This is NOT a bug in timezone handling or production bot.
   It's a natural consequence of data snapshot timing.
""")

print("=" * 80)
