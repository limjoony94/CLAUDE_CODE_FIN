"""
ë©”ì¸ë„· ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ (Testnet ì•„ë‹˜!)

BingX ë©”ì¸ë„·ì—ì„œ ì‹¤ì œ ê±°ë˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
OHLCV ë°ì´í„°ëŠ” publicì´ë¯€ë¡œ API í‚¤ ë¶ˆí•„ìš”.
"""

import sys
from pathlib import Path
import time
from datetime import datetime, timedelta

import ccxt
import pandas as pd
from loguru import logger

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))


def main():
    logger.info("="*60)
    logger.info("MAINNET DATA COLLECTION")
    logger.info("="*60)
    logger.info("âš ï¸ Collecting REAL mainnet data (NOT testnet!)")
    logger.info("")

    # BingX ë©”ì¸ë„· ì—°ê²° (API í‚¤ ë¶ˆí•„ìš” - public data)
    exchange = ccxt.bingx({
        'enableRateLimit': True,
        'options': {'defaultType': 'swap'},
    })

    # ë©”ì¸ë„· ëª¨ë“œ (sandbox ë¹„í™œì„±í™”)
    logger.info("âœ… Mainnet mode enabled")

    symbol = 'BTC/USDT:USDT'
    timeframe = '5m'

    # í…ŒìŠ¤íŠ¸: ìµœê·¼ 30ì¼ë¶€í„° ì‹œì‘ (ë” ì•ˆì •ì )
    days = 100
    logger.info(f"Attempting to collect {days} days of {timeframe} mainnet data")

    end_date = datetime.now()
    start_date = end_date - timedelta(days=days)
    since = int(start_date.timestamp() * 1000)

    logger.info(f"Date range: {start_date} â†’ {end_date}")

    all_candles = []
    batch_count = 0
    max_batches = 50  # ìµœëŒ€ 50íšŒ ìš”ì²­ (50,000 ìº”ë“¤)

    try:
        while batch_count < max_batches:
            logger.info(f"Batch {batch_count + 1}/{max_batches} - From {datetime.fromtimestamp(since/1000)}")

            candles = exchange.fetch_ohlcv(
                symbol=symbol,
                timeframe=timeframe,
                since=since,
                limit=1000
            )

            if not candles:
                logger.info("No more data available")
                break

            all_candles.extend(candles)
            batch_count += 1

            logger.info(f"  Fetched {len(candles)} candles (Total: {len(all_candles)})")

            last_timestamp = candles[-1][0]

            if last_timestamp >= int(end_date.timestamp() * 1000):
                logger.info("Reached current time")
                break

            since = last_timestamp + 1
            time.sleep(exchange.rateLimit / 1000)

    except Exception as e:
        logger.error(f"Error: {str(e)}")
        import traceback
        traceback.print_exc()

    if not all_candles:
        logger.error("No data collected!")
        return

    # DataFrame ë³€í™˜
    df = pd.DataFrame(all_candles, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
    df = df.drop_duplicates(subset=['timestamp'], keep='last')
    df = df.sort_values('timestamp').reset_index(drop=True)

    # ì €ì¥ (ê¸°ì¡´ íŒŒì¼ ë®ì–´ì“°ê¸°)
    save_dir = project_root / 'data' / 'historical'
    filepath = save_dir / 'BTCUSDT_5m_max.csv'

    # ë°±ì—… ìƒì„±
    if filepath.exists():
        backup_path = save_dir / f'BTCUSDT_5m_max_testnet_backup_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv'
        logger.info(f"ğŸ“¦ Backing up old data to: {backup_path.name}")
        import shutil
        shutil.copy(filepath, backup_path)

    df.to_csv(filepath, index=False)

    actual_days = (df['timestamp'].max() - df['timestamp'].min()).total_seconds() / 86400

    logger.info("\n" + "="*60)
    logger.info("MAINNET DATA COLLECTION COMPLETE")
    logger.info("="*60)
    logger.info(f"Source:         BingX MAINNET (Real trading data)")
    logger.info(f"Total Candles:  {len(df):>10,}")
    logger.info(f"Date Range:     {df['timestamp'].min()} â†’ {df['timestamp'].max()}")
    logger.info(f"Actual Days:    {actual_days:>10.2f}")
    logger.info(f"Saved to:       {filepath}")
    logger.info("="*60)
    logger.info("")
    logger.info("âœ… Mainnet data is now ready for training!")
    logger.info("âš ï¸ This data reflects REAL market conditions")


if __name__ == "__main__":
    main()
