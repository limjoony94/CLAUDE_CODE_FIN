"""
Retrain Entry Models with NEW 120 FEATURES - Walk-Forward Decoupled
====================================================================

Uses complete feature engineering overhaul:
- 40 Multi-Timeframe features
- 16 Market Regime features
- 23 Momentum Quality features
- 20 Microstructure features
- 21 Dynamic Pattern features

Total: 120 NEW features + original OHLCV/indicators

Training Methodology:
  âœ… Walk-Forward Validation (no look-ahead bias)
  âœ… Filtered Simulation (83% faster)
  âœ… Decoupled Training (breaks circular dependency)

Created: 2025-10-29
"""

import sys
from pathlib import Path
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import pandas as pd
import numpy as np
import joblib
from datetime import datetime
import xgboost as xgb
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import precision_score, recall_score, f1_score

# Configuration
ENTRY_THRESHOLD = 0.75
ML_EXIT_THRESHOLD = 0.75
LEVERAGE = 4
EMERGENCY_STOP_LOSS = -0.03
EMERGENCY_MAX_HOLD = 120

DATA_DIR = PROJECT_ROOT / "data" / "features"
MODELS_DIR = PROJECT_ROOT / "models"

print("="*80)
print("ENTRY MODEL RETRAINING - NEW 120 FEATURES")
print("="*80)
print()
print("New Feature Categories:")
print("  âœ… Multi-Timeframe: 40 features")
print("  âœ… Market Regime: 16 features")
print("  âœ… Momentum Quality: 23 features")
print("  âœ… Microstructure: 20 features")
print("  âœ… Dynamic Patterns: 21 features")
print("  Total: 120 NEW features")
print()
print(f"Training Configuration:")
print(f"  Entry Threshold: {ENTRY_THRESHOLD}")
print(f"  ML Exit Threshold: {ML_EXIT_THRESHOLD}")
print(f"  Emergency Stop Loss: {EMERGENCY_STOP_LOSS}")
print(f"  Emergency Max Hold: {EMERGENCY_MAX_HOLD} candles")
print()

# ==============================================================================
# STEP 1: Load New Features Dataset
# ==============================================================================

print("-"*80)
print("STEP 1: Loading NEW Features Dataset")
print("-"*80)

df = pd.read_csv(DATA_DIR / "BTCUSDT_5m_pattern_features.csv")
print(f"âœ… Loaded {len(df):,} candles with {len(df.columns)} columns")
print(f"   Date range: {df['timestamp'].iloc[0]} to {df['timestamp'].iloc[-1]}")
print()

# Handle NaN values from rolling calculations
print("Handling NaN values...")
initial_rows = len(df)
df = df.ffill().bfill()  # Forward/backward fill
df = df.dropna()  # Drop any remaining NaN rows
print(f"âœ… Cleaned: {len(df):,} candles ({initial_rows - len(df):,} rows with NaN removed)")
print()

# ==============================================================================
# STEP 2: Load Exit Models for Trade Simulation
# ==============================================================================

print("-"*80)
print("STEP 2: Loading Exit Models")
print("-"*80)

# Load LONG Exit model
long_exit_model = joblib.load(MODELS_DIR / "xgboost_long_exit_threshold_075_20251027_190512.pkl")
long_exit_scaler = joblib.load(MODELS_DIR / "xgboost_long_exit_threshold_075_20251027_190512_scaler.pkl")
with open(MODELS_DIR / "xgboost_long_exit_threshold_075_20251027_190512_features.txt", 'r') as f:
    long_exit_features = [line.strip() for line in f if line.strip()]

# Load SHORT Exit model
short_exit_model = joblib.load(MODELS_DIR / "xgboost_short_exit_threshold_075_20251027_190512.pkl")
short_exit_scaler = joblib.load(MODELS_DIR / "xgboost_short_exit_threshold_075_20251027_190512_scaler.pkl")
with open(MODELS_DIR / "xgboost_short_exit_threshold_075_20251027_190512_features.txt", 'r') as f:
    short_exit_features = [line.strip() for line in f if line.strip()]

print(f"âœ… LONG Exit: {len(long_exit_features)} features")
print(f"âœ… SHORT Exit: {len(short_exit_features)} features")
print()

# ==============================================================================
# STEP 3: Prepare Exit Features (for simulation)
# ==============================================================================

print("-"*80)
print("STEP 3: Preparing Exit Features")
print("-"*80)

def prepare_exit_features(df):
    """Calculate exit-specific features"""
    df = df.copy()

    # Volume features
    df['volume_ratio'] = df['volume'] / df['volume'].rolling(20).mean()
    df['volume_surge'] = (df['volume'] > df['volume'].rolling(20).mean() * 1.5).astype(float)

    # Price vs MA features
    if 'ma_50' not in df.columns and 'close' in df.columns:
        df['ma_50'] = df['close'].rolling(50).mean()
    if 'ma_200' not in df.columns and 'close' in df.columns:
        df['ma_200'] = df['close'].rolling(200).mean()

    df['price_vs_ma20'] = (df['close'] - df['ma_50']) / df['ma_50'] if 'ma_50' in df.columns else 0
    df['price_vs_ma50'] = (df['close'] - df['ma_200']) / df['ma_200'] if 'ma_200' in df.columns else 0

    # Volatility features
    df['returns'] = df['close'].pct_change()
    df['volatility_20'] = df['returns'].rolling(20).std()
    df['volatility_regime'] = (df['volatility_20'] > df['volatility_20'].rolling(100).median()).astype(float)

    # RSI features
    if 'rsi' in df.columns:
        df['rsi_slope'] = df['rsi'].diff(3) / 3
        df['rsi_overbought'] = (df['rsi'] > 70).astype(float)
        df['rsi_oversold'] = (df['rsi'] < 30).astype(float)
    else:
        df['rsi_slope'] = 0
        df['rsi_overbought'] = 0
        df['rsi_oversold'] = 0

    df['rsi_divergence'] = 0  # Placeholder

    # MACD features
    if 'macd_hist' in df.columns:
        df['macd_histogram_slope'] = df['macd_hist'].diff(3) / 3
    else:
        df['macd_histogram_slope'] = 0

    if 'macd' in df.columns and 'macd_signal' in df.columns:
        df['macd_crossover'] = ((df['macd'] > df['macd_signal']) &
                                (df['macd'].shift(1) <= df['macd_signal'].shift(1))).astype(float)
        df['macd_crossunder'] = ((df['macd'] < df['macd_signal']) &
                                 (df['macd'].shift(1) >= df['macd_signal'].shift(1))).astype(float)
    else:
        df['macd_crossover'] = 0
        df['macd_crossunder'] = 0

    # Price pattern features
    df['higher_high'] = (df['high'] > df['high'].shift(1)).astype(float)
    df['lower_low'] = (df['low'] < df['low'].shift(1)).astype(float)
    df['price_acceleration'] = df['close'].diff().diff()

    # Support/Resistance
    df['near_resistance'] = 0  # Placeholder
    df['near_support'] = 0  # Placeholder

    # Bollinger Bands position
    if 'bb_upper' in df.columns and 'bb_lower' in df.columns:
        df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])
    else:
        df['bb_position'] = 0.5

    df = df.ffill().bfill()
    return df

df = prepare_exit_features(df)
print("âœ… Exit features prepared")
print()

# ==============================================================================
# STEP 4: Filter Entry Candidates
# ==============================================================================

def filter_entry_candidates(df, side):
    """
    Pre-filter candidates using simple heuristics
    Reduces candidates by ~83%
    """
    print(f"\nðŸ” Filtering {side} Entry Candidates...")

    mask = pd.Series(True, index=df.index)

    # Volume filter
    if 'volume' in df.columns:
        volume_filter = df['volume'] > df['volume'].rolling(20).mean()
        mask = mask & volume_filter

    # RSI filter
    if 'rsi' in df.columns:
        rsi_filter = (df['rsi'] > 30) & (df['rsi'] < 70)
        mask = mask & rsi_filter

    # MACD filter
    if 'macd' in df.columns and 'macd_signal' in df.columns:
        if side == 'LONG':
            macd_filter = df['macd'] > df['macd_signal']
        else:
            macd_filter = df['macd'] < df['macd_signal']
        mask = mask & macd_filter

    # Trend filter (using Multi-Timeframe features!)
    if 'mtf_1h_trend_strength' in df.columns:
        if side == 'LONG':
            trend_filter = df['mtf_1h_trend_strength'] > 0
        else:
            trend_filter = df['mtf_1h_trend_strength'] < 0
        mask = mask & trend_filter

    candidates = df[mask].copy()
    reduction_pct = (1 - len(candidates) / len(df)) * 100

    print(f"  Original: {len(df):,} candles")
    print(f"  Filtered: {len(candidates):,} candidates")
    print(f"  Reduction: {reduction_pct:.1f}%")

    return candidates

# ==============================================================================
# STEP 5: Simulate Trades and Generate Labels
# ==============================================================================

def simulate_trade_outcome(entry_row, df_future, side, exit_model, exit_scaler, exit_features):
    """
    Simulate trade outcome for a single entry candidate
    Returns: (label, hold_time, pnl_pct, exit_reason)
    """
    entry_price = entry_row['close']
    max_hold = EMERGENCY_MAX_HOLD

    # Get future candles (up to max_hold)
    entry_idx = entry_row.name
    df_slice = df_future.loc[entry_idx:entry_idx+max_hold].iloc[1:]  # Next candles only

    if len(df_slice) == 0:
        return (0, 0, 0, 'NO_DATA')

    # Simulate candle by candle
    for i, (idx, row) in enumerate(df_slice.iterrows(), 1):
        current_price = row['close']

        # Calculate P&L
        if side == 'LONG':
            price_change_pct = (current_price - entry_price) / entry_price
        else:
            price_change_pct = (entry_price - current_price) / entry_price

        leveraged_pnl_pct = price_change_pct * LEVERAGE

        # Check Emergency Stop Loss
        if leveraged_pnl_pct <= EMERGENCY_STOP_LOSS:
            return (0, i, leveraged_pnl_pct, 'STOP_LOSS')

        # Check Emergency Max Hold
        if i >= max_hold:
            return (int(leveraged_pnl_pct > 0), i, leveraged_pnl_pct, 'MAX_HOLD')

        # Check ML Exit
        try:
            X_exit = row[exit_features].values.reshape(1, -1)
            X_exit_scaled = exit_scaler.transform(X_exit)
            exit_prob = exit_model.predict_proba(X_exit_scaled)[0][1]

            if exit_prob >= ML_EXIT_THRESHOLD:
                label = int(leveraged_pnl_pct > 0)
                return (label, i, leveraged_pnl_pct, 'ML_EXIT')
        except:
            # If exit feature missing, continue
            pass

    # Max hold reached
    final_pnl = leveraged_pnl_pct
    return (int(final_pnl > 0), max_hold, final_pnl, 'MAX_HOLD')

def generate_entry_labels(candidates, df, side, exit_model, exit_scaler, exit_features):
    """
    Generate Entry labels for all candidates via trade simulation
    """
    print(f"\nðŸ“Š Generating {side} Entry Labels...")

    labels = []
    hold_times = []
    pnls = []
    exit_reasons = []

    for idx, row in candidates.iterrows():
        label, hold_time, pnl, reason = simulate_trade_outcome(
            row, df, side, exit_model, exit_scaler, exit_features
        )
        labels.append(label)
        hold_times.append(hold_time)
        pnls.append(pnl)
        exit_reasons.append(reason)

    candidates['label'] = labels
    candidates['hold_time'] = hold_times
    candidates['pnl_pct'] = pnls
    candidates['exit_reason'] = exit_reasons

    positive_rate = (candidates['label'].sum() / len(candidates)) * 100
    avg_hold = candidates['hold_time'].mean()

    print(f"  âœ… Labels generated: {len(candidates):,}")
    print(f"     Positive rate: {positive_rate:.2f}%")
    print(f"     Avg hold time: {avg_hold:.1f} candles")

    return candidates

# ==============================================================================
# STEP 6: Walk-Forward Decoupled Training
# ==============================================================================

def train_entry_model(candidates, side, fold_name):
    """
    Train Entry model using Walk-Forward Decoupled methodology
    """
    print(f"\nðŸŽ¯ Training {side} Entry Model - {fold_name}")

    # Select features (all NEW features + original indicators)
    feature_prefixes = ['mtf_', 'regime_', 'momentum_', 'microstructure_', 'pattern_']
    all_features = [col for col in candidates.columns
                   if any(col.startswith(p) for p in feature_prefixes)
                   or col in ['rsi', 'macd', 'macd_signal', 'macd_hist', 'bb_upper', 'bb_lower',
                             'volume', 'returns', 'atr', 'adx']]

    # Remove features with too many NaN
    valid_features = []
    for feat in all_features:
        if feat in candidates.columns:
            nan_pct = candidates[feat].isnull().sum() / len(candidates)
            if nan_pct < 0.1:  # Less than 10% NaN
                valid_features.append(feat)

    print(f"  Features: {len(valid_features)} (from {len(all_features)} candidates)")

    # Prepare data
    X = candidates[valid_features].copy()
    y = candidates['label'].copy()

    # Handle any remaining NaN
    X = X.ffill().bfill().fillna(0)

    # Scale features
    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X)

    # Train XGBoost model
    model = xgb.XGBClassifier(
        n_estimators=200,
        max_depth=6,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        scale_pos_weight=1.0,
        eval_metric='logloss'
    )

    model.fit(X_scaled, y)

    # Evaluate
    y_pred = model.predict(X_scaled)
    y_proba = model.predict_proba(X_scaled)[:, 1]

    precision = precision_score(y, y_pred, zero_division=0)
    recall = recall_score(y, y_pred, zero_division=0)
    f1 = f1_score(y, y_pred, zero_division=0)

    # Prediction rate at threshold 0.75
    pred_rate = (y_proba >= ENTRY_THRESHOLD).sum() / len(y_proba) * 100

    print(f"  âœ… Training complete")
    print(f"     Precision: {precision:.3f}")
    print(f"     Recall: {recall:.3f}")
    print(f"     F1 Score: {f1:.3f}")
    print(f"     Prediction Rate (â‰¥0.75): {pred_rate:.2f}%")

    return model, scaler, valid_features, pred_rate

# ==============================================================================
# MAIN TRAINING LOOP
# ==============================================================================

print("-"*80)
print("STEP 7: Walk-Forward Training (5 Folds)")
print("-"*80)

tscv = TimeSeriesSplit(n_splits=5)

# Initialize results storage
long_results = {'fold': [], 'pred_rate': [], 'label_rate': []}
short_results = {'fold': [], 'pred_rate': [], 'label_rate': []}

# Train LONG Entry
print("\n" + "="*80)
print("TRAINING LONG ENTRY MODEL")
print("="*80)

best_long_pred_rate = 0
best_long_model = None
best_long_scaler = None
best_long_features = None
best_long_fold = None

for fold_idx, (train_idx, val_idx) in enumerate(tscv.split(df), 1):
    print(f"\n{'='*80}")
    print(f"FOLD {fold_idx}/5")
    print(f"{'='*80}")

    df_train = df.iloc[train_idx].copy()

    # Filter candidates
    long_candidates = filter_entry_candidates(df_train, 'LONG')

    # Generate labels
    long_candidates = generate_entry_labels(
        long_candidates, df_train, 'LONG',
        long_exit_model, long_exit_scaler, long_exit_features
    )

    # Train model
    model, scaler, features, pred_rate = train_entry_model(
        long_candidates, 'LONG', f"Fold {fold_idx}"
    )

    # Track results
    label_rate = (long_candidates['label'].sum() / len(long_candidates)) * 100
    long_results['fold'].append(fold_idx)
    long_results['pred_rate'].append(pred_rate)
    long_results['label_rate'].append(label_rate)

    # Keep best fold
    if pred_rate > best_long_pred_rate:
        best_long_pred_rate = pred_rate
        best_long_model = model
        best_long_scaler = scaler
        best_long_features = features
        best_long_fold = fold_idx

print(f"\n{'='*80}")
print(f"BEST LONG MODEL: Fold {best_long_fold} (Pred Rate: {best_long_pred_rate:.2f}%)")
print(f"{'='*80}")

# Save LONG model
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
long_model_path = MODELS_DIR / f"xgboost_long_entry_newfeatures_{timestamp}.pkl"
long_scaler_path = MODELS_DIR / f"xgboost_long_entry_newfeatures_{timestamp}_scaler.pkl"
long_features_path = MODELS_DIR / f"xgboost_long_entry_newfeatures_{timestamp}_features.txt"

joblib.dump(best_long_model, long_model_path)
joblib.dump(best_long_scaler, long_scaler_path)
with open(long_features_path, 'w') as f:
    for feat in best_long_features:
        f.write(f"{feat}\n")

print(f"âœ… LONG Entry model saved")
print(f"   Model: {long_model_path.name}")
print(f"   Features: {len(best_long_features)}")
print()

# Train SHORT Entry
print("\n" + "="*80)
print("TRAINING SHORT ENTRY MODEL")
print("="*80)

best_short_pred_rate = 0
best_short_model = None
best_short_scaler = None
best_short_features = None
best_short_fold = None

for fold_idx, (train_idx, val_idx) in enumerate(tscv.split(df), 1):
    print(f"\n{'='*80}")
    print(f"FOLD {fold_idx}/5")
    print(f"{'='*80}")

    df_train = df.iloc[train_idx].copy()

    # Filter candidates
    short_candidates = filter_entry_candidates(df_train, 'SHORT')

    # Generate labels
    short_candidates = generate_entry_labels(
        short_candidates, df_train, 'SHORT',
        short_exit_model, short_exit_scaler, short_exit_features
    )

    # Train model
    model, scaler, features, pred_rate = train_entry_model(
        short_candidates, 'SHORT', f"Fold {fold_idx}"
    )

    # Track results
    label_rate = (short_candidates['label'].sum() / len(short_candidates)) * 100
    short_results['fold'].append(fold_idx)
    short_results['pred_rate'].append(pred_rate)
    short_results['label_rate'].append(label_rate)

    # Keep best fold
    if pred_rate > best_short_pred_rate:
        best_short_pred_rate = pred_rate
        best_short_model = model
        best_short_scaler = scaler
        best_short_features = features
        best_short_fold = fold_idx

print(f"\n{'='*80}")
print(f"BEST SHORT MODEL: Fold {best_short_fold} (Pred Rate: {best_short_pred_rate:.2f}%)")
print(f"{'='*80}")

# Save SHORT model
short_model_path = MODELS_DIR / f"xgboost_short_entry_newfeatures_{timestamp}.pkl"
short_scaler_path = MODELS_DIR / f"xgboost_short_entry_newfeatures_{timestamp}_scaler.pkl"
short_features_path = MODELS_DIR / f"xgboost_short_entry_newfeatures_{timestamp}_features.txt"

joblib.dump(best_short_model, short_model_path)
joblib.dump(best_short_scaler, short_scaler_path)
with open(short_features_path, 'w') as f:
    for feat in best_short_features:
        f.write(f"{feat}\n")

print(f"âœ… SHORT Entry model saved")
print(f"   Model: {short_model_path.name}")
print(f"   Features: {len(best_short_features)}")
print()

# ==============================================================================
# SUMMARY
# ==============================================================================

print("="*80)
print("TRAINING COMPLETE")
print("="*80)
print()

print("LONG Entry Model:")
print(f"  Best Fold: {best_long_fold}/5")
print(f"  Prediction Rate (â‰¥0.75): {best_long_pred_rate:.2f}%")
print(f"  Features: {len(best_long_features)}")
print()

print("SHORT Entry Model:")
print(f"  Best Fold: {best_short_fold}/5")
print(f"  Prediction Rate (â‰¥0.75): {best_short_pred_rate:.2f}%")
print(f"  Features: {len(best_short_features)}")
print()

print("Label Consistency (across folds):")
print(f"  LONG: {np.mean(long_results['label_rate']):.2f}% Â± {np.std(long_results['label_rate']):.2f}%")
print(f"  SHORT: {np.mean(short_results['label_rate']):.2f}% Â± {np.std(short_results['label_rate']):.2f}%")
print()

print("Next Step: Run backtest at threshold 0.75 to validate 3-8 trades/day target")
print()
