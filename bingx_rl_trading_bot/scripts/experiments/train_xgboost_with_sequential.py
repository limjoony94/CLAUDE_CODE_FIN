"""XGBoost Regression with Sequential Features - ì‚¬ìš©ì í†µì°° ê²€ì¦

ì‚¬ìš©ì í†µì°°: "ëª¨ë¸ì´ ê°€ì¥ ìµœê·¼ ìº”ë“¤ì˜ ì§€í‘œë§Œ ë³´ê³  ì¶”ì„¸ë¥¼ ëª¨ë¥¸ë‹¤"

í•´ê²° ë°©ì•ˆ:
1. Sequential/Context Features ì¶”ê°€ (ì¶”ì„¸, ë³€í™”, íŒ¨í„´)
2. XGBoost íšŒê·€ ëª¨ë¸ ì§ì ‘ í›ˆë ¨ (prepare_features ìš°íšŒ)
3. ì˜ˆì¸¡ê°’ ë‹¤ì–‘ì„± ê²€ì¦ (ë‹¨ì¼ ìƒìˆ˜ ë¬¸ì œ í•´ê²° ì—¬ë¶€)
4. Buy & Holdì™€ ë¹„êµ ê²€ì¦
"""

import sys
from pathlib import Path

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import pandas as pd
import numpy as np
from loguru import logger
import xgboost as xgb
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

from src.indicators.technical_indicators import TechnicalIndicators


def prepare_sequential_data(df: pd.DataFrame, lookahead: int = 48):
    """Sequential Featuresë¥¼ í™œìš©í•œ ë°ì´í„° ì¤€ë¹„"""
    data = df.copy()

    # íƒ€ê²Ÿ: ë¯¸ë˜ ìˆ˜ìµë¥ 
    data['target'] = data['close'].pct_change(lookahead).shift(-lookahead)

    # Feature columns (OHLCV ë° timestamp ì œì™¸)
    exclude_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'target']
    feature_cols = [col for col in data.columns if col not in exclude_cols]

    # ê²°ì¸¡ì¹˜ ì œê±°
    data = data.dropna()

    logger.info(f"Target Statistics:")
    logger.info(f"  Mean: {data['target'].mean()*100:.3f}%")
    logger.info(f"  Std: {data['target'].std()*100:.3f}%")
    logger.info(f"  Min: {data['target'].min()*100:.2f}%")
    logger.info(f"  Max: {data['target'].max()*100:.2f}%")

    logger.info(f"\nTotal Features: {len(feature_cols)}")
    logger.info(f"Sample Features: {feature_cols[:10]}")

    return data, feature_cols


def main():
    logger.info("="*80)
    logger.info("XGBoost Regression with Sequential Features")
    logger.info("ì‚¬ìš©ì í†µì°° ê²€ì¦: ì¶”ì„¸/ë¬¸ë§¥ ì •ë³´ ì¶”ê°€ë¡œ ì˜ˆì¸¡ ê°œì„ ë˜ëŠ”ê°€?")
    logger.info("="*80)

    # 1. ë°ì´í„° ë¡œë“œ
    logger.info("\n1. Loading data...")
    data_file = project_root / 'data' / 'historical' / 'BTCUSDT_5m_max.csv'
    df = pd.read_csv(data_file)
    logger.info(f"Loaded {len(df)} candles")

    # 2. ê¸°ë³¸ ì§€í‘œ ê³„ì‚°
    logger.info("\n2. Calculating base indicators...")
    indicators = TechnicalIndicators()
    df_processed = indicators.calculate_all_indicators(df)
    logger.info(f"Base indicators calculated: {len(df_processed)} rows")

    # 3. Sequential Features ì¶”ê°€
    logger.info("\n3. Adding Sequential Features...")
    logger.info("   â†’ RSI ë³€í™” (5, 20)")
    logger.info("   â†’ ê°€ê²© vs MA ë¹„ìœ¨")
    logger.info("   â†’ MACD í¬ë¡œìŠ¤ì˜¤ë²„")
    logger.info("   â†’ ì—°ì† ìº”ë“¤ íŒ¨í„´")
    logger.info("   â†’ ì¶”ì„¸ ì •ë ¬")

    df_sequential = indicators.calculate_sequential_features(df_processed)

    original_features = len(df_processed.columns)
    new_features = len(df_sequential.columns)
    logger.info(f"\nFeature Count: {original_features} â†’ {new_features} (+{new_features - original_features})")

    # 4. ë°ì´í„° ì¤€ë¹„
    logger.info("\n4. Preparing data with targets...")
    data, feature_cols = prepare_sequential_data(df_sequential, lookahead=48)

    # 5. Train/Validation/Test Split
    logger.info("\n5. Splitting data...")
    n = len(data)
    train_end = int(n * 0.7)
    val_end = int(n * 0.85)

    train_df = data.iloc[:train_end].copy()
    val_df = data.iloc[train_end:val_end].copy()
    test_df = data.iloc[val_end:].copy()

    logger.info(f"Train: {len(train_df)} candles")
    logger.info(f"Validation: {len(val_df)} candles")
    logger.info(f"Test: {len(test_df)} candles")
    logger.info(f"Test Period: {test_df['timestamp'].iloc[0]} to {test_df['timestamp'].iloc[-1]}")

    # 6. XGBoost í›ˆë ¨
    logger.info("\n6. Training XGBoost Regression...")

    X_train = train_df[feature_cols].values
    y_train = train_df['target'].values

    X_val = val_df[feature_cols].values
    y_val = val_df['target'].values

    X_test = test_df[feature_cols].values
    y_test = test_df['target'].values

    # DMatrix ìƒì„±
    dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=feature_cols)
    dval = xgb.DMatrix(X_val, label=y_val, feature_names=feature_cols)
    dtest = xgb.DMatrix(X_test, label=y_test, feature_names=feature_cols)

    # ëª¨ë¸ íŒŒë¼ë¯¸í„°
    params = {
        'objective': 'reg:squarederror',
        'max_depth': 6,
        'learning_rate': 0.05,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'reg_alpha': 0.1,
        'reg_lambda': 1.0,
        'eval_metric': 'rmse',
        'tree_method': 'hist',
        'random_state': 42
    }

    # í›ˆë ¨
    evals = [(dtrain, 'train'), (dval, 'val')]
    evals_result = {}

    logger.info("Training XGBoost model...")
    model = xgb.train(
        params,
        dtrain,
        num_boost_round=200,
        evals=evals,
        early_stopping_rounds=30,
        evals_result=evals_result,
        verbose_eval=20
    )

    logger.info(f"\nTraining completed - Best iteration: {model.best_iteration}")

    # 7. ì˜ˆì¸¡ê°’ ë¶„ì„ (ğŸš¨ í•µì‹¬ ê²€ì¦: ë‹¨ì¼ ìƒìˆ˜ ë¬¸ì œ í•´ê²° ì—¬ë¶€)
    logger.info("\n7. Analyzing Predictions...")

    train_preds = model.predict(dtrain)
    val_preds = model.predict(dval)
    test_preds = model.predict(dtest)

    logger.info(f"\nTrain Predictions:")
    logger.info(f"  Mean: {train_preds.mean()*100:.6f}%")
    logger.info(f"  Std: {train_preds.std()*100:.6f}%")
    logger.info(f"  Min: {train_preds.min()*100:.6f}%")
    logger.info(f"  Max: {train_preds.max()*100:.6f}%")

    logger.info(f"\nValidation Predictions:")
    logger.info(f"  Mean: {val_preds.mean()*100:.6f}%")
    logger.info(f"  Std: {val_preds.std()*100:.6f}%")
    logger.info(f"  Min: {val_preds.min()*100:.6f}%")
    logger.info(f"  Max: {val_preds.max()*100:.6f}%")

    logger.info(f"\nTest Predictions:")
    logger.info(f"  Mean: {test_preds.mean()*100:.6f}%")
    logger.info(f"  Std: {test_preds.std()*100:.6f}%")
    logger.info(f"  Min: {test_preds.min()*100:.6f}%")
    logger.info(f"  Max: {test_preds.max()*100:.6f}%")

    # ğŸš¨ í•µì‹¬ ê²€ì¦: í‘œì¤€í¸ì°¨ê°€ 0ì¸ê°€?
    logger.info("\n" + "="*80)
    logger.info("ğŸ” CRITICAL VALIDATION: Constant Prediction Problem Solved?")
    logger.info("="*80)

    if test_preds.std() < 0.0001:
        logger.error("\nğŸš¨ FAILURE: Predictions are still constant!")
        logger.error("   Sequential Features did NOT solve the problem.")
        logger.error(f"   All predictions â‰ˆ {test_preds.mean()*100:.6f}%")
    else:
        logger.success(f"\nâœ… SUCCESS: Predictions have variance!")
        logger.success(f"   Std = {test_preds.std()*100:.6f}%")
        logger.success(f"   Range: {test_preds.min()*100:.3f}% to {test_preds.max()*100:.3f}%")
        logger.success("   Sequential Features SOLVED the constant prediction problem!")

    # 8. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
    logger.info("\n8. Model Performance...")

    test_rmse = np.sqrt(mean_squared_error(y_test, test_preds))
    test_mae = mean_absolute_error(y_test, test_preds)
    test_r2 = r2_score(y_test, test_preds)

    logger.info(f"\nTest Set Regression Metrics:")
    logger.info(f"  RMSE: {test_rmse*100:.3f}%")
    logger.info(f"  MAE: {test_mae*100:.3f}%")
    logger.info(f"  RÂ²: {test_r2:.4f}")

    # 9. ì‹ í˜¸ ìƒì„± ë° ë°±í…ŒìŠ¤íŒ…
    logger.info("\n9. Generating Trading Signals...")

    long_threshold = 0.015  # 1.5%
    short_threshold = -0.015  # -1.5%

    signals = np.zeros(len(test_preds))
    signals[test_preds > long_threshold] = 1  # LONG
    signals[test_preds < short_threshold] = -1  # SHORT

    unique, counts = np.unique(signals, return_counts=True)
    logger.info(f"\nSignal Distribution:")
    for sig, count in zip(unique, counts):
        sig_name = {-1: 'SHORT', 0: 'HOLD', 1: 'LONG'}[int(sig)]
        logger.info(f"  {sig_name}: {count}/{len(signals)} ({count/len(signals)*100:.1f}%)")

    # ê°„ë‹¨í•œ ë°±í…ŒìŠ¤íŒ…
    logger.info("\n10. Backtesting...")

    balance = 10000.0
    position = 0.0
    num_trades = 0

    for i in range(len(test_df)):
        signal = signals[i]
        price = test_df.iloc[i]['close']

        if signal == 1 and position == 0:  # LONG ì§„ì…
            position = 0.03
            num_trades += 1
        elif signal == -1 and position == 0:  # SHORT ì§„ì…
            position = -0.03
            num_trades += 1
        elif signal == 0 and position != 0:  # ì²­ì‚°
            position = 0

    # ìµœì¢… ìˆ˜ìµë¥  (ê°„ì†Œí™”)
    first_price = test_df.iloc[0]['close']
    last_price = test_df.iloc[-1]['close']

    # Buy & Hold ë¹„êµ
    bh_return = (last_price - first_price) / first_price * 100

    logger.info(f"\nBacktest Results (Simplified):")
    logger.info(f"  Trades Generated: {num_trades}")
    logger.info(f"  Buy & Hold Return: {bh_return:+.2f}%")

    # 11. ë¹„êµ ë¶„ì„
    logger.info("\n" + "="*80)
    logger.info("COMPARISON: Sequential Features vs Original Models")
    logger.info("="*80)

    logger.info("\n| Model | Prediction Std | Trades | RÂ² | Note |")
    logger.info("|-------|---------------|--------|-----|------|")
    logger.info("| REGRESSION (Original) | 0.0000% | 0 | -0.15 | Single constant |")
    logger.info(f"| REGRESSION (Sequential) | {test_preds.std()*100:.4f}% | {num_trades:6d} | {test_r2:.2f} | **This model** |")

    # 12. ìµœì¢… ê²°ë¡ 
    logger.info("\n" + "="*80)
    logger.info("FINAL CONCLUSION")
    logger.info("="*80)

    logger.info("\nì‚¬ìš©ì ê°€ì„¤: 'ëª¨ë¸ì´ ê°€ì¥ ìµœê·¼ ìº”ë“¤ì˜ ì§€í‘œë§Œ ë³´ê³  ì¶”ì„¸ë¥¼ ëª¨ë¥¸ë‹¤'")
    logger.info("í•´ê²° ë°©ì•ˆ: Sequential/Context Features ì¶”ê°€")

    if test_preds.std() > 0.0001:
        logger.success("\nâœ… ì‚¬ìš©ì í†µì°°ì´ ì •í™•í–ˆìŠµë‹ˆë‹¤!")
        logger.success("   Sequential Featuresê°€ ì˜ˆì¸¡ ë‹¤ì–‘ì„±ì„ íšŒë³µì‹œì¼°ìŠµë‹ˆë‹¤!")
        logger.info(f"\nê°œì„  ì‚¬í•­:")
        logger.info(f"  â€¢ Prediction Std: 0.0000% â†’ {test_preds.std()*100:.4f}%")
        logger.info(f"  â€¢ RÂ²: -0.15 â†’ {test_r2:.2f}")
        logger.info(f"  â€¢ Trades: 0 â†’ {num_trades}")

        if test_r2 > 0:
            logger.success(f"\nğŸ‰ ëª¨ë¸ì´ ì‹¤ì œ ì˜ˆì¸¡ ëŠ¥ë ¥ì„ íšë“í–ˆìŠµë‹ˆë‹¤! (RÂ² > 0)")
        else:
            logger.warning(f"\nâš ï¸ ì˜ˆì¸¡ ë‹¤ì–‘ì„±ì€ ìˆìœ¼ë‚˜ RÂ² < 0 (ì—¬ì „íˆ baseline ë¯¸ë‹¬)")
            logger.info("   ì¶”ê°€ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì´ë‚˜ ë‹¤ë¥¸ ì ‘ê·¼ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        logger.error("\nâŒ Sequential Featuresë¡œë„ ë¬¸ì œê°€ í•´ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        logger.info("\nê°€ëŠ¥í•œ ì›ì¸:")
        logger.info("  â€¢ ì¶”ê°€ëœ í”¼ì²˜ë„ ì—¬ì „íˆ ë‹¨ì¼ ì‹œì  íŠ¹ì„±")
        logger.info("  â€¢ ëª¨ë¸ì´ ë¯¸ë˜ ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•˜ë‹¤ê³  í•™ìŠµ")
        logger.info("  â€¢ 5ë¶„ë´‰ ì‹œì¥ì˜ ë³¸ì§ˆì  ë¬´ì‘ìœ„ì„±")

    logger.info("\n" + "="*80)
    logger.info("âœ… Analysis Complete!")
    logger.info("="*80)


if __name__ == "__main__":
    main()
