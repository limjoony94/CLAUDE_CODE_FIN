"""
Phase 4 Dual Entry + Dual Exit Model TESTNET Trading Bot

Î™©Ìëú: 4-Model Ï†ÑÎûµ Ïã§ÏãúÍ∞Ñ Í≤ÄÏ¶ù (Entry Dual + Exit Dual)
- Entry Models:
  * LONG Model: XGBoost Phase 4 (ÏÉÅÏäπ ÏòàÏ∏° Ï†ÑÏö©)
  * SHORT Model: XGBoost Short (ÌïòÎùΩ ÏòàÏ∏° Ï†ÑÏö©)
- Exit Models:
  * LONG Exit Model: ML-learned optimal exit timing for LONG positions
  * SHORT Exit Model: ML-learned optimal exit timing for SHORT positions
- Dynamic Position Sizing (20-95% adaptive)
- Ïã§Ï†ú BingX Testnet API Ï£ºÎ¨∏ Ïã§Ìñâ
- 5Î∂Ñ Ï∫îÎì§ Í∏∞Î∞ò

‚ö†Ô∏è Ï§ëÏöî: Ïù¥Í≤ÉÏùÄ TESTNET TRADINGÏûÖÎãàÎã§!
- Ïã§Ï†ú Ï£ºÎ¨∏ Ï†úÏ∂ú (Í∞ÄÏÉÅ ÏûêÎ≥∏)
- Ïã§Ï†ú Í≥ÑÏ¢å ÏûîÍ≥† ÏÇ¨Ïö©
- Ïã§Ï†ú Ìè¨ÏßÄÏÖò Í¥ÄÎ¶¨
- Paper TradingÏù¥ ÏïÑÎãò!

Expected Performance (from ML Exit Model backtesting):
- Returns: +2.85% per 2 days (+39.2% vs rule-based)
- Win Rate: 94.7% (+5.0% vs rule-based 89.7%)
- Avg Holding: 2.36 hours (-41% vs rule-based 4.0h)
- Exit Efficiency: 87.6% ML Exit, 12.4% Max Hold
- Entry Distribution: 87.6% LONG, 12.4% SHORT
"""

import os
import time
import json
import pickle
from datetime import datetime, timedelta
from pathlib import Path
import pandas as pd
import numpy as np
from loguru import logger
import yaml

# Project paths
PROJECT_ROOT = Path(__file__).parent.parent.parent

MODELS_DIR = PROJECT_ROOT / "models"
RESULTS_DIR = PROJECT_ROOT / "results"
LOGS_DIR = PROJECT_ROOT / "logs"
CONFIG_DIR = PROJECT_ROOT / "config"

# Import project modules
import sys
sys.path.insert(0, str(PROJECT_ROOT))
from scripts.production.train_xgboost_improved_v3_phase2 import calculate_features
from scripts.production.advanced_technical_features import AdvancedTechnicalFeatures
from scripts.production.dynamic_position_sizing import DynamicPositionSizer
from src.api.bingx_client import BingXClient
from src.api.exceptions import (
    BingXAPIError,
    BingXOrderError,
    BingXInsufficientBalanceError
)
from src.utils.institutional_logger import InstitutionalLogger

# Create directories
for dir_path in [RESULTS_DIR, LOGS_DIR]:
    dir_path.mkdir(parents=True, exist_ok=True)

# Logging setup
log_file = LOGS_DIR / f"phase4_dynamic_testnet_trading_{datetime.now().strftime('%Y%m%d')}.log"
logger.add(log_file, rotation="1 day", retention="30 days")

# ============================================================================
# Bot Singleton - Prevent Multiple Instances
# ============================================================================

class BotSingleton:
    """
    Singleton pattern to ensure only one bot instance runs at a time

    Prevents duplicate bot execution that causes:
    - Conflicting orders
    - Statistical confusion
    - Increased API rate limit violations
    """

    def __init__(self):
        self.lock_file = None
        self.lock_path = RESULTS_DIR / "bot_instance.lock"

    def __enter__(self):
        """Acquire exclusive lock on bot execution"""
        try:
            # Create lock file
            self.lock_file = open(self.lock_path, 'w')

            # Try to acquire exclusive lock (non-blocking)
            # This will fail if another instance already holds the lock
            if sys.platform == 'win32':
                # Windows: Use msvcrt for file locking
                import msvcrt
                try:
                    msvcrt.locking(self.lock_file.fileno(), msvcrt.LK_NBLCK, 1)
                except IOError:
                    logger.error("=" * 80)
                    logger.error("‚ùå BOT ALREADY RUNNING!")
                    logger.error("=" * 80)
                    logger.error("Another instance of the bot is currently active.")
                    logger.error("Only ONE bot instance can run at a time to prevent:")
                    logger.error("  - Duplicate order execution")
                    logger.error("  - API rate limit violations")
                    logger.error("  - Statistical tracking conflicts")
                    logger.error("")
                    logger.error("Check running processes:")
                    logger.error("  ps aux | grep phase4_dynamic_testnet_trading")
                    logger.error("=" * 80)
                    sys.exit(1)
            else:
                # Unix/Linux: Use fcntl for file locking
                import fcntl
                try:
                    fcntl.flock(self.lock_file.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
                except IOError:
                    logger.error("=" * 80)
                    logger.error("‚ùå BOT ALREADY RUNNING!")
                    logger.error("=" * 80)
                    logger.error("Another instance of the bot is currently active.")
                    logger.error("Only ONE bot instance can run at a time.")
                    logger.error("")
                    logger.error("Check running processes:")
                    logger.error("  ps aux | grep phase4_dynamic_testnet_trading")
                    logger.error("=" * 80)
                    sys.exit(1)

            # Write PID to lock file
            self.lock_file.write(f"{os.getpid()}\n")
            self.lock_file.write(f"{datetime.now().isoformat()}\n")
            self.lock_file.flush()

            logger.success("‚úÖ Bot instance lock acquired")

        except Exception as e:
            logger.error(f"Failed to acquire bot lock: {e}")
            sys.exit(1)

        return self

    def __exit__(self, *args):
        """Release lock on bot exit"""
        if self.lock_file:
            try:
                # Release lock
                if sys.platform == 'win32':
                    import msvcrt
                    msvcrt.locking(self.lock_file.fileno(), msvcrt.LK_UNLCK, 1)
                else:
                    import fcntl
                    fcntl.flock(self.lock_file.fileno(), fcntl.LOCK_UN)

                self.lock_file.close()

                # Remove lock file
                if self.lock_path.exists():
                    self.lock_path.unlink()

                logger.info("‚úÖ Bot instance lock released")

            except Exception as e:
                logger.error(f"Error releasing bot lock: {e}")

# ============================================================================
# Phase 4 Dynamic Testnet Configuration
# ============================================================================

class Phase4TestnetConfig:
    """Phase 4 Dual Model Testnet Configuration"""

    # XGBoost Thresholds (2025-10-15: DYNAMIC THRESHOLD SYSTEM - Root Cause Solution)
    # HISTORY:
    # - Previous: LONG 0.80, SHORT 0.50 (asymmetric labels - Train-Test Mismatch)
    # - Fixed: LONG 0.80, SHORT 0.80 (symmetric labels - 12.29% return)
    # - Entry Optimized: LONG 0.70, SHORT 0.65 (19.88% return, +62% improvement!)
    # - V2 Optimization (2 weeks): All parameters (35.67% return, +79% vs entry-only!)
    # - V3 Optimization (3 months): Same parameters validated (97.82% train return, robust!)
    # - 2025-10-15 17:40: DYNAMIC THRESHOLD SYSTEM (adaptive to market regime changes)

    # Base Thresholds (optimized from V3, used as baseline for dynamic adjustment)
    BASE_LONG_ENTRY_THRESHOLD = 0.70   # V3 optimal for average market (10.1% signal rate)
    BASE_SHORT_ENTRY_THRESHOLD = 0.65  # V3 optimal for average market
    EXIT_THRESHOLD = 0.70  # Exit Model threshold (0.70 = optimal from V3 backtest) - UNCHANGED

    # Dynamic Threshold Configuration
    ENABLE_DYNAMIC_THRESHOLD = True  # Enable adaptive threshold system
    EXPECTED_SIGNAL_RATE = 0.101     # 10.1% average signal rate from V3 backtest
    TARGET_TRADES_PER_WEEK = 42.5    # Target trade frequency (from V3 backtest)
    DYNAMIC_LOOKBACK_HOURS = 6       # Monitor recent 6-hour signal rate
    THRESHOLD_ADJUSTMENT_FACTOR = 0.15  # Max adjustment: ¬±0.15 (allows 0.55-0.85 range)
    MIN_THRESHOLD = 0.55             # Minimum allowed threshold (prevent over-trading)
    MAX_THRESHOLD = 0.85             # Maximum allowed threshold (prevent under-trading)

    # Expected Metrics (2025-10-15: V3 FULL-DATASET OPTIMIZATION - Temporal Bias Eliminated)
    # Backtest Results (3-month dataset with walk-forward validation):
    # - Training: 97.82% return (9.0 weeks), Sharpe 31.00
    # - Validation: 7.60% return (1.9 weeks), Sharpe 25.06
    # - Test (out-of-sample): 28.66% return (1.9 weeks), Sharpe 16.60
    # - Win Rate: 82.9% (validated on 3-month data, not just 2 weeks)
    # - Trades/Week: 42.5 (test set, similar to V2 but more robust)
    # - Avg Position: 71.6% (validated across multiple market regimes)
    # - Max Drawdown: -8.43% (V3 test set)
    # - Dataset: 3x more data than V2, eliminates Oct 10 outlier bias
    EXPECTED_RETURN_PER_WEEK = 14.86  # From V3 test set (28.66% / 1.9 weeks)
    EXPECTED_WIN_RATE = 82.9  # V3 test set result (out-of-sample validated)
    EXPECTED_TRADES_PER_WEEK = 42.5  # V3 test set (validated on diverse conditions)
    EXPECTED_SHARPE_RATIO = 16.60  # V3 test set (robust, out-of-sample)
    EXPECTED_MAX_DRAWDOWN = -8.43  # V3 test set
    EXPECTED_AVG_POSITION = 71.6  # average position size %
    EXPECTED_AVG_HOLDING = 1.53  # hours (unchanged)
    EXPECTED_LONG_RATIO = 91.7  # % of LONG trades (expected)
    EXPECTED_SHORT_RATIO = 8.3  # % of SHORT trades (expected)

    # Targets
    TARGET_WIN_RATE = 60.0
    TARGET_VS_BH = 10.0
    TARGET_AVG_POSITION = (40.0, 70.0)  # reasonable range

    # API Configuration (BingX Testnet) - Load from YAML config
    @staticmethod
    def _load_api_keys():
        """Load API keys from config/api_keys.yaml"""
        api_keys_file = CONFIG_DIR / "api_keys.yaml"
        if api_keys_file.exists():
            with open(api_keys_file, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                return config.get('bingx', {}).get('testnet', {})
        return {}

    _api_config = _load_api_keys()
    API_KEY = _api_config.get('api_key', os.getenv("BINGX_API_KEY", ""))
    API_SECRET = _api_config.get('secret_key', os.getenv("BINGX_API_SECRET", ""))
    USE_TESTNET = True  # ALWAYS testnet for this bot!

    # Trading Parameters (2025-10-15: EXIT OPTIMIZED)
    SYMBOL = "BTC-USDT"
    TIMEFRAME = "5m"
    STOP_LOSS = 0.01  # 1% (optimal from 81 combinations backtest)
    TAKE_PROFIT = 0.02  # 2% (optimized: 3% ‚Üí 2%, early profit taking strategy!)
    MAX_HOLDING_HOURS = 4  # 4 hours (optimal from backtest)

    # Dynamic Position Sizing Parameters (2025-10-15: V3 FULL-DATASET OPTIMIZATION)
    # Optimized on 3-month dataset with walk-forward validation (27 weight + 6 position combos)
    # Key findings:
    # - Same optimal parameters as V2, but validated on 6x more data (robust!)
    # - Eliminates temporal bias from Oct 10 outlier (V2 used 11.46% signal rate, V3 uses 5.46%)
    # - Streak factor 2.5√ó more important than expected (0.10 ‚Üí 0.25)
    # - Out-of-sample test confirms: 28.66% return, 82.9% win rate, Sharpe 16.60
    BASE_POSITION_PCT = 0.65  # 65% base (V3 validated: best across all market regimes)
    MAX_POSITION_PCT = 0.95   # 95% maximum (V3 validated: optimal risk management)
    MIN_POSITION_PCT = 0.20   # 20% minimum (V3 validated: unchanged, optimal)
    SIGNAL_WEIGHT = 0.35      # 35% (V3 validated: avoid over-reliance on signal)
    VOLATILITY_WEIGHT = 0.25  # 25% (V3 validated: balanced volatility impact)
    REGIME_WEIGHT = 0.15      # 15% (V3 validated: moderate regime impact)
    STREAK_WEIGHT = 0.25      # 25% (V3 validated: manages consecutive losses effectively)

    # Risk Management
    MAX_DAILY_LOSS_PCT = 0.05
    TRANSACTION_COST = 0.0006

    # Data Collection
    LOOKBACK_CANDLES = 1440  # CRITICAL: Match backtest window size (72 hours = 5 days)
    # Previously 500 (42h), but backtests used 1440 (72h) windows
    # Model learned patterns from 72h context, so must use 72h in production
    UPDATE_INTERVAL = 300  # 5 minutes

    # Market Regime Classification
    BULL_THRESHOLD = 3.0
    BEAR_THRESHOLD = -2.0

    # Order Configuration
    ORDER_TYPE = "MARKET"  # MARKET or LIMIT
    LEVERAGE = 4  # Optimized from backtest: Dynamic @ 4x = 12.06% per 5 days (vs 7.68% baseline)

# ============================================================================
# Phase 4 Dynamic Testnet Trading Bot
# ============================================================================

class Phase4DynamicTestnetTradingBot:
    """
    Phase 4 Dynamic Position Sizing TESTNET Trading Bot

    Ïã§Ï†ú BingX Testnet APIÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Ï£ºÎ¨∏ Ïã§Ìñâ Î∞è Ìè¨ÏßÄÏÖò Í¥ÄÎ¶¨
    """

    def __init__(self):
        # Initialize BingX Client
        if not Phase4TestnetConfig.API_KEY or not Phase4TestnetConfig.API_SECRET:
            raise ValueError("BingX API credentials not found! Set BINGX_API_KEY and BINGX_API_SECRET environment variables.")

        self.client = BingXClient(
            api_key=Phase4TestnetConfig.API_KEY,
            secret_key=Phase4TestnetConfig.API_SECRET,
            testnet=Phase4TestnetConfig.USE_TESTNET,
            timeout=30
        )
        logger.success("‚úÖ BingX Testnet Client initialized")

        # Initialize Institutional Logger
        session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.inst_logger = InstitutionalLogger(
            log_dir=LOGS_DIR,
            strategy_name="Phase4_Dynamic_4Model",
            session_id=session_id,
            initial_capital=0.0,  # Will be set after balance query
            enable_json=True,
            enable_text=True,
            enable_audit=True
        )
        logger.success("‚úÖ Institutional Logger initialized")
        # Note: log_system_event not available in InstitutionalLogger
        logger.info("Bot initialization: Phase 4 Dynamic (4-Model System)")

        # Test connection
        if not self.client.ping():
            raise ConnectionError("Failed to connect to BingX Testnet API")
        logger.success("‚úÖ BingX Testnet API connection verified")

        # Load DUAL MODELS: LONG + SHORT (with MinMaxScaler normalization)
        # LONG Model + Scaler
        long_model_path = MODELS_DIR / "xgboost_v4_phase4_advanced_lookahead3_thresh0.pkl"
        long_scaler_path = MODELS_DIR / "xgboost_v4_phase4_advanced_lookahead3_thresh0_scaler.pkl"
        feature_path = MODELS_DIR / "xgboost_v4_phase4_advanced_lookahead3_thresh0_features.txt"

        if not long_model_path.exists():
            raise FileNotFoundError(f"XGBoost LONG model not found: {long_model_path}")
        if not long_scaler_path.exists():
            raise FileNotFoundError(f"XGBoost LONG scaler not found: {long_scaler_path}")

        with open(long_model_path, 'rb') as f:
            self.long_model = pickle.load(f)

        with open(long_scaler_path, 'rb') as f:
            self.long_scaler = pickle.load(f)

        with open(feature_path, 'r') as f:
            self.feature_columns = [line.strip() for line in f.readlines()]

        logger.success(f"‚úÖ XGBoost LONG model loaded: {len(self.feature_columns)} features")
        logger.success(f"‚úÖ XGBoost LONG scaler loaded: MinMaxScaler(-1, 1)")

        # SHORT Model + Scaler
        short_model_path = MODELS_DIR / "xgboost_short_model_lookahead3_thresh0.3.pkl"
        short_scaler_path = MODELS_DIR / "xgboost_short_model_lookahead3_thresh0.3_scaler.pkl"

        if not short_model_path.exists():
            raise FileNotFoundError(f"XGBoost SHORT model not found: {short_model_path}")
        if not short_scaler_path.exists():
            raise FileNotFoundError(f"XGBoost SHORT scaler not found: {short_scaler_path}")

        with open(short_model_path, 'rb') as f:
            self.short_model = pickle.load(f)

        with open(short_scaler_path, 'rb') as f:
            self.short_scaler = pickle.load(f)

        logger.success(f"‚úÖ XGBoost SHORT model loaded: {len(self.feature_columns)} features")
        logger.success(f"‚úÖ XGBoost SHORT scaler loaded: MinMaxScaler(-1, 1)")
        logger.info(f"üìä Dual Model Strategy: LONG + SHORT (independent predictions, normalized features)")

        # EXIT Models (LONG + SHORT - trained for optimal exit timing, with Scalers)
        long_exit_model_path = MODELS_DIR / "xgboost_v4_long_exit.pkl"
        long_exit_scaler_path = MODELS_DIR / "xgboost_v4_long_exit_scaler.pkl"
        short_exit_model_path = MODELS_DIR / "xgboost_v4_short_exit.pkl"
        short_exit_scaler_path = MODELS_DIR / "xgboost_v4_short_exit_scaler.pkl"
        exit_features_path = MODELS_DIR / "xgboost_v4_long_exit_features.txt"

        if not long_exit_model_path.exists():
            raise FileNotFoundError(f"XGBoost LONG EXIT model not found: {long_exit_model_path}")
        if not long_exit_scaler_path.exists():
            raise FileNotFoundError(f"XGBoost LONG EXIT scaler not found: {long_exit_scaler_path}")
        if not short_exit_model_path.exists():
            raise FileNotFoundError(f"XGBoost SHORT EXIT model not found: {short_exit_model_path}")
        if not short_exit_scaler_path.exists():
            raise FileNotFoundError(f"XGBoost SHORT EXIT scaler not found: {short_exit_scaler_path}")

        with open(long_exit_model_path, 'rb') as f:
            self.long_exit_model = pickle.load(f)

        with open(long_exit_scaler_path, 'rb') as f:
            self.long_exit_scaler = pickle.load(f)

        with open(short_exit_model_path, 'rb') as f:
            self.short_exit_model = pickle.load(f)

        with open(short_exit_scaler_path, 'rb') as f:
            self.short_exit_scaler = pickle.load(f)

        with open(exit_features_path, 'r') as f:
            self.exit_feature_columns = [line.strip() for line in f.readlines()]

        logger.success(f"‚úÖ XGBoost LONG EXIT model loaded (44 features: 36 base + 8 position)")
        logger.success(f"‚úÖ XGBoost LONG EXIT scaler loaded: MinMaxScaler(-1, 1)")
        logger.success(f"‚úÖ XGBoost SHORT EXIT model loaded (44 features: 36 base + 8 position)")
        logger.success(f"‚úÖ XGBoost SHORT EXIT scaler loaded: MinMaxScaler(-1, 1)")
        logger.info(f"üìä Exit Strategy: ML-based optimal timing (LONG/SHORT specialized, threshold=0.75, normalized features)")

        # Initialize Advanced Technical Features
        self.adv_features = AdvancedTechnicalFeatures(lookback_sr=50, lookback_trend=20)
        logger.success("‚úÖ Advanced Technical Features initialized")

        # Initialize Dynamic Position Sizer
        self.position_sizer = DynamicPositionSizer(
            base_position_pct=Phase4TestnetConfig.BASE_POSITION_PCT,
            max_position_pct=Phase4TestnetConfig.MAX_POSITION_PCT,
            min_position_pct=Phase4TestnetConfig.MIN_POSITION_PCT,
            signal_weight=Phase4TestnetConfig.SIGNAL_WEIGHT,
            volatility_weight=Phase4TestnetConfig.VOLATILITY_WEIGHT,
            regime_weight=Phase4TestnetConfig.REGIME_WEIGHT,
            streak_weight=Phase4TestnetConfig.STREAK_WEIGHT
        )
        logger.success("‚úÖ Dynamic Position Sizer initialized")
        logger.info(f"   Position Range: {Phase4TestnetConfig.MIN_POSITION_PCT*100:.0f}% - {Phase4TestnetConfig.MAX_POSITION_PCT*100:.0f}%")
        logger.info(f"   Base Position: {Phase4TestnetConfig.BASE_POSITION_PCT*100:.0f}%")

        # Balance caching (reduce API calls and rate limiting)
        self._cached_balance = None
        self._balance_cache_time = None
        self._balance_cache_ttl = 240.0  # 4 minutes TTL (covers full 5-min cycle)

        # Rate limit tracking
        self._rate_limit_hit_count = 0
        self._last_rate_limit_time = None

        # Get initial balance with retry logic for initialization
        logger.info("üìä Getting initial account balance...")
        max_init_attempts = 3
        for init_attempt in range(max_init_attempts):
            try:
                self.initial_balance = self._get_account_balance(use_cache=False)
                logger.success(f"‚úÖ Testnet Account Balance: ${self.initial_balance:,.2f} USDT")

                # Update institutional logger with initial capital
                self.inst_logger.initial_capital = self.initial_balance
                # Note: log_system_event not available in InstitutionalLogger
                logger.info(f"Initial balance set: ${self.initial_balance:,.2f} USDT")
                break
            except Exception as e:
                if init_attempt < max_init_attempts - 1:
                    wait_time = 120 * (init_attempt + 1)  # 2 min, 4 min
                    logger.warning(f"‚ö†Ô∏è Failed to get initial balance (attempt {init_attempt+1}/{max_init_attempts})")
                    logger.warning(f"   Error: {e}")
                    logger.info(f"   Waiting {wait_time}s before retry...")
                    time.sleep(wait_time)
                else:
                    logger.error(f"‚ùå Failed to get initial balance after {max_init_attempts} attempts")
                    logger.error(f"   This usually indicates API rate limiting or connectivity issues")
                    logger.error(f"   Please wait 5-10 minutes and try again")
                    # Note: log_error signature: (error_type, error_message, stack_trace)
                    import traceback
                    self.inst_logger.log_error(
                        error_type="INITIALIZATION_ERROR",
                        error_message=f"Failed to retrieve initial balance: {str(e)}",
                        stack_trace=traceback.format_exc()
                    )
                    raise

        # Trading state
        self.trades = []
        self.session_start = datetime.now()
        self.market_regime_history = []

        # Try to load previous session state (for session continuity)
        self._load_previous_state()

        # Signal tracking (for opportunity cost analysis)
        self.signal_log = []

        # Buy & Hold tracking
        self.bh_btc_quantity = 0.0
        self.bh_entry_price = 0.0
        self.bh_initialized = False

        logger.info("=" * 80)
        logger.info("Phase 4 Dual Entry + Dual Exit Model Testnet Trading Bot Initialized")
        logger.info("=" * 80)
        logger.info(f"Network: BingX TESTNET ‚úÖ (Real Order Execution!)")
        logger.info(f"Entry Strategy: Dual Model (LONG + SHORT independent predictions)")
        logger.info(f"Exit Strategy: Dual ML Exit Model @ {Phase4TestnetConfig.EXIT_THRESHOLD:.2f} (LONG/SHORT specialized)")
        logger.info(f"Initial Balance: ${self.initial_balance:,.2f} USDT")
        logger.info(f"")
        logger.info(f"Expected Performance (2025-10-15: V3 FULL-DATASET OPTIMIZATION - Temporal Bias Eliminated):")
        logger.info(f"  - Returns: +{Phase4TestnetConfig.EXPECTED_RETURN_PER_WEEK:.2f}% per week (V3 out-of-sample validated)")
        logger.info(f"  - Win Rate: {Phase4TestnetConfig.EXPECTED_WIN_RATE:.1f}% (validated on 3 months data)")
        logger.info(f"  - Avg Holding: {Phase4TestnetConfig.EXPECTED_AVG_HOLDING:.2f} hours")
        logger.info(f"  - Trades/Week: {Phase4TestnetConfig.EXPECTED_TRADES_PER_WEEK:.1f} (robust across market regimes)")
        logger.info(f"  - Avg Position: {Phase4TestnetConfig.EXPECTED_AVG_POSITION:.1f}%")
        logger.info(f"  - Max Drawdown: {Phase4TestnetConfig.EXPECTED_MAX_DRAWDOWN:.2f}%")
        logger.info(f"  - Sharpe Ratio: {Phase4TestnetConfig.EXPECTED_SHARPE_RATIO:.2f} (out-of-sample)")
        logger.info(f"  - Dataset: 3-month validation (eliminates Oct 10 outlier bias)")
        logger.info(f"")
        logger.info(f"Entry Thresholds (Dynamic System 2025-10-15):")
        logger.info(f"  - LONG Base: {Phase4TestnetConfig.BASE_LONG_ENTRY_THRESHOLD:.2f} (adaptive range: {Phase4TestnetConfig.MIN_THRESHOLD:.2f}-{Phase4TestnetConfig.MAX_THRESHOLD:.2f})")
        logger.info(f"  - SHORT Base: {Phase4TestnetConfig.BASE_SHORT_ENTRY_THRESHOLD:.2f} (adaptive range: {Phase4TestnetConfig.MIN_THRESHOLD:.2f}-{Phase4TestnetConfig.MAX_THRESHOLD:.2f})")
        logger.info(f"  - Dynamic Adjustment: {'ENABLED' if Phase4TestnetConfig.ENABLE_DYNAMIC_THRESHOLD else 'DISABLED'} (lookback: {Phase4TestnetConfig.DYNAMIC_LOOKBACK_HOURS}h)")
        logger.info(f"")
        logger.info(f"Exit Parameters (Optimized 2025-10-15):")
        logger.info(f"  - ML Exit Threshold: {Phase4TestnetConfig.EXIT_THRESHOLD:.2f}")
        logger.info(f"  - Stop Loss: {Phase4TestnetConfig.STOP_LOSS*100:.1f}%")
        logger.info(f"  - Take Profit: {Phase4TestnetConfig.TAKE_PROFIT*100:.1f}%")
        logger.info(f"  - Max Holding: {Phase4TestnetConfig.MAX_HOLDING_HOURS}h")
        logger.info(f"")
        logger.info(f"Position Sizing (V3 Optimized 2025-10-15 - Full-Dataset Validated):")
        logger.info(f"  - Base: {Phase4TestnetConfig.BASE_POSITION_PCT*100:.0f}% | Max: {Phase4TestnetConfig.MAX_POSITION_PCT*100:.0f}% | Min: {Phase4TestnetConfig.MIN_POSITION_PCT*100:.0f}%")
        logger.info(f"  - Weights: Signal={Phase4TestnetConfig.SIGNAL_WEIGHT:.2f} | Vol={Phase4TestnetConfig.VOLATILITY_WEIGHT:.2f} | Regime={Phase4TestnetConfig.REGIME_WEIGHT:.2f} | Streak={Phase4TestnetConfig.STREAK_WEIGHT:.2f}")
        logger.info(f"  - V3 Validation: Same as V2 but tested on 3-month dataset (6x more data)")
        logger.info(f"  - Key Insight: Streak factor 2.5√ó more important, manages consecutive losses!")
        logger.info("=" * 80)

    def _load_previous_state(self):
        """Load previous session state for session continuity"""
        state_file = RESULTS_DIR / "phase4_testnet_trading_state.json"

        if not state_file.exists():
            logger.info("üÜï Starting fresh session (no previous state)")
            return

        try:
            with open(state_file, 'r') as f:
                prev_state = json.load(f)

            # Check if continuing same session (within 30 minutes)
            prev_session_start_str = prev_state.get('session_start')
            if not prev_session_start_str:
                logger.info("üÜï Starting fresh session (no session_start in previous state)")
                return

            prev_session_start = datetime.fromisoformat(prev_session_start_str)
            time_since_prev = (datetime.now() - prev_session_start).total_seconds()

            # Get previous session's initial balance
            prev_initial_balance = prev_state.get('initial_balance')
            current_balance_at_restart = self.initial_balance  # Current balance (already set in __init__)

            # If within 4 hours (matches Max Holding time), consider it the same session
            if time_since_prev < 14400:  # 4 hours = 14400 seconds
                time_str = f"{time_since_prev/60:.1f} minutes" if time_since_prev < 3600 else f"{time_since_prev/3600:.1f} hours"

                # ‚úÖ FIX: Log restart details for debugging
                logger.info("=" * 80)
                logger.success(f"üîÑ BOT RESTART DETECTED")
                logger.info(f"   Previous session started: {time_str} ago")
                logger.info(f"   Previous initial balance: ${prev_initial_balance:,.2f} USDT")
                logger.info(f"   Current balance: ${current_balance_at_restart:,.2f} USDT")

                # Calculate P&L since previous session start
                if prev_initial_balance:
                    session_pnl = current_balance_at_restart - prev_initial_balance
                    session_pnl_pct = (session_pnl / prev_initial_balance) * 100
                    logger.info(f"   Session P&L: ${session_pnl:+,.2f} ({session_pnl_pct:+.2f}%)")

                logger.info("=" * 80)

                # ‚úÖ FIX: Restore original initial_balance (not current balance!)
                if prev_initial_balance is not None:
                    self.initial_balance = prev_initial_balance
                    logger.success(f"‚úÖ Restored original initial balance: ${self.initial_balance:,.2f} USDT")
                else:
                    logger.warning("‚ö†Ô∏è No initial_balance in state file, using current balance")
                    logger.warning(f"   This will cause ROI calculation inaccuracy!")

                # Restore trades list with datetime deserialization
                if 'trades' in prev_state and isinstance(prev_state['trades'], list):
                    self.trades = []
                    for trade_data in prev_state['trades']:
                        # Deserialize datetime fields
                        if 'entry_time' in trade_data and isinstance(trade_data['entry_time'], str):
                            trade_data['entry_time'] = datetime.fromisoformat(trade_data['entry_time'])
                        if 'exit_time' in trade_data and isinstance(trade_data['exit_time'], str):
                            trade_data['exit_time'] = datetime.fromisoformat(trade_data['exit_time'])

                        # ‚úÖ FIX: Convert probability string to float (fixes numpy type error)
                        if 'probability' in trade_data and isinstance(trade_data['probability'], str):
                            trade_data['probability'] = float(trade_data['probability'])

                        # ‚úÖ FIX: Convert sizing_factors strings to float
                        if 'sizing_factors' in trade_data and isinstance(trade_data['sizing_factors'], dict):
                            for key, value in trade_data['sizing_factors'].items():
                                if isinstance(value, str):
                                    trade_data['sizing_factors'][key] = float(value)

                        self.trades.append(trade_data)

                    open_trades = [t for t in self.trades if t.get('status') == 'OPEN']
                    closed_trades = [t for t in self.trades if t.get('status') == 'CLOSED']
                    logger.info(f"   Restored {len(self.trades)} trades ({len(open_trades)} open, {len(closed_trades)} closed)")

                    # Log open trade details
                    for trade in open_trades:
                        entry_time = trade.get('entry_time')
                        hours_held = (datetime.now() - entry_time).total_seconds() / 3600 if entry_time else 0
                        logger.info(f"   Open Position: {trade.get('quantity', 0):.4f} BTC @ ${trade.get('entry_price', 0):,.2f} ({hours_held:.1f}h held)")
                else:
                    logger.info("   No trades to restore")

                # Update session_start to maintain original session time
                self.session_start = prev_session_start

            else:
                logger.info(f"üÜï Starting fresh session (previous session {time_since_prev/3600:.1f} hours ago)")

        except Exception as e:
            logger.error(f"Failed to load previous state: {e}")
            logger.info("üÜï Starting fresh session")

    def _is_rate_limit_error(self, error) -> bool:
        """Check if error is a rate limit error"""
        error_str = str(error).lower()
        return ('100410' in error_str or
                'rate limit' in error_str or
                'frequency limit' in error_str or
                'please try again later' in error_str)

    def _handle_rate_limit_error(self, error):
        """
        Handle rate limit error with appropriate wait time

        BingX rate limit errors include unblock timestamp in the response.
        We extract this and wait appropriately.
        """
        self._rate_limit_hit_count += 1
        self._last_rate_limit_time = datetime.now()

        # Try to extract unblock timestamp from error message
        # Format: "unblocked after 1760369709268"
        error_str = str(error)
        wait_time = 300  # Default 5 minutes

        try:
            if 'unblocked after' in error_str:
                import re
                match = re.search(r'unblocked after (\d+)', error_str)
                if match:
                    unblock_timestamp_ms = int(match.group(1))
                    unblock_time = datetime.fromtimestamp(unblock_timestamp_ms / 1000)
                    wait_time = max(0, (unblock_time - datetime.now()).total_seconds())
                    wait_time = min(wait_time, 600)  # Cap at 10 minutes
        except Exception:
            pass  # Use default wait time

        logger.error("=" * 80)
        logger.error("üö® BingX API RATE LIMIT EXCEEDED!")
        logger.error("=" * 80)
        logger.error(f"Rate limit hit #{self._rate_limit_hit_count} this session")
        logger.error(f"Error: {error}")
        logger.error(f"")
        logger.error(f"Required Action:")
        logger.error(f"  ‚è≥ Waiting {wait_time:.0f} seconds ({wait_time/60:.1f} minutes) for rate limit to reset...")
        logger.error(f"")
        logger.error(f"Root Cause:")
        logger.error(f"  - Too many API calls in short time period")
        logger.error(f"  - BingX enforces strict rate limits on testnet")
        logger.error(f"")
        logger.error(f"Prevention:")
        logger.error(f"  ‚úÖ Balance cache TTL increased to 4 minutes")
        logger.error(f"  ‚úÖ Retry logic reduced from 3 to 2 attempts")
        logger.error(f"  ‚úÖ Longer wait times between retries")
        logger.error("=" * 80)

        # Log rate limit error to institutional logger
        self.inst_logger.log_compliance_event(
            event_type="API_RATE_LIMIT",
            description=f"BingX API rate limit exceeded (hit #{self._rate_limit_hit_count})",
            severity="HIGH",
            action_taken=f"Waiting {wait_time:.0f}s for rate limit reset"
        )
        # Note: log_error signature: (error_type, error_message, stack_trace)
        self.inst_logger.log_error(
            error_type="API_RATE_LIMIT",
            error_message=f"BingX API rate limit exceeded (hit #{self._rate_limit_hit_count}): {str(error)}"
        )

        # Wait for rate limit to reset
        time.sleep(wait_time)

        logger.success(f"‚úÖ Rate limit wait completed, resuming operations...")

    def _get_account_balance(self, retry_count: int = 2, retry_delay: float = 60.0, use_cache: bool = True) -> float:
        """
        Get USDT balance from account with retry logic and caching

        Args:
            retry_count: Number of retry attempts (default: 2, reduced from 3)
            retry_delay: Initial delay between retries in seconds (default: 60s, increased from 2s)
            use_cache: Whether to use cached balance if available (default: True)

        Returns:
            USDT balance

        Raises:
            Exception: If all retry attempts fail and no cache available
        """
        # Check cache first (if enabled and valid)
        if use_cache and self._cached_balance is not None and self._balance_cache_time is not None:
            cache_age = (datetime.now() - self._balance_cache_time).total_seconds()
            if cache_age < self._balance_cache_ttl:
                logger.debug(f"Using cached balance (age: {cache_age:.1f}s / {self._balance_cache_ttl:.0f}s TTL)")
                return self._cached_balance

        last_error = None

        for attempt in range(retry_count):
            try:
                balance_data = self.client.get_balance()

                # BingX balance response format
                if isinstance(balance_data, dict):
                    balance = balance_data.get('balance', {})
                    usdt_balance = float(balance.get('balance', 0.0))

                    # Validate balance (must be > 0 for trading)
                    if usdt_balance > 0:
                        # Update cache on successful query
                        self._cached_balance = usdt_balance
                        self._balance_cache_time = datetime.now()
                        return usdt_balance
                    else:
                        logger.warning(f"Balance returned 0 (attempt {attempt+1}/{retry_count})")

                elif isinstance(balance_data, list):
                    for item in balance_data:
                        if item.get('asset') == 'USDT':
                            usdt_balance = float(item.get('balance', 0.0))
                            if usdt_balance > 0:
                                # Update cache on successful query
                                self._cached_balance = usdt_balance
                                self._balance_cache_time = datetime.now()
                                return usdt_balance
                            else:
                                logger.warning(f"Balance returned 0 (attempt {attempt+1}/{retry_count})")

                else:
                    logger.warning(f"Could not parse balance response (attempt {attempt+1}/{retry_count})")

            except Exception as e:
                last_error = e

                # Check if it's a rate limit error
                if self._is_rate_limit_error(e):
                    logger.warning(f"Rate limit error on attempt {attempt+1}/{retry_count}")
                    self._handle_rate_limit_error(e)
                    # After handling rate limit, continue to next attempt
                    continue
                else:
                    logger.warning(f"Balance query failed (attempt {attempt+1}/{retry_count}): {e}")

            # Wait before retry (exponential backoff for non-rate-limit errors)
            if attempt < retry_count - 1:
                wait_time = retry_delay * (2 ** attempt)
                logger.info(f"Retrying in {wait_time:.1f}s...")
                time.sleep(wait_time)

        # All retries failed - try to return cached balance if available
        if self._cached_balance is not None:
            cache_age = (datetime.now() - self._balance_cache_time).total_seconds()
            logger.warning(f"‚ö†Ô∏è Using cached balance after {retry_count} failed attempts (age: {cache_age:.1f}s)")
            logger.warning(f"   Cached value may be stale, but continuing to prevent bot crash")
            return self._cached_balance

        # No cache available - raise exception
        error_msg = f"Failed to get valid balance after {retry_count} attempts"
        if last_error:
            error_msg += f": {last_error}"
        logger.error(error_msg)
        raise Exception(error_msg)

    def _get_current_position(self) -> dict:
        """Get current open position"""
        try:
            positions = self.client.get_positions(Phase4TestnetConfig.SYMBOL)

            for pos in positions:
                position_amt = float(pos.get('positionAmt', 0))
                if position_amt != 0:  # Has open position
                    return {
                        'symbol': pos.get('symbol'),
                        'position_side': pos.get('positionSide'),
                        'position_amt': abs(position_amt),
                        'entry_price': float(pos.get('entryPrice', 0)),
                        'unrealized_pnl': float(pos.get('unrealizedProfit', 0)),
                        'leverage': int(float(pos.get('leverage', 1)))
                    }

            return None  # No position

        except Exception as e:
            logger.error(f"Failed to get positions: {e}")
            return None

    def run(self):
        """Main trading loop"""
        logger.info("üöÄ Starting Phase 4 Dynamic Testnet Trading...")
        logger.info(f"‚ö†Ô∏è WARNING: This bot will execute REAL orders on Testnet!")
        logger.info(f"Update: Synchronized to 5-minute candle completion")

        try:
            while True:
                self._update_cycle()

                # Sleep until next 5-minute candle completes
                now = datetime.now()
                current_minute = now.minute
                current_second = now.second

                # Next candle completion time (00, 05, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55)
                next_candle_minute = ((current_minute // 5) + 1) * 5
                if next_candle_minute >= 60:
                    next_candle_minute = 0

                # Calculate seconds to wait
                minutes_to_wait = (next_candle_minute - current_minute) % 60
                seconds_to_wait = minutes_to_wait * 60 - current_second + 5  # +5 sec buffer after candle completes

                if seconds_to_wait <= 0:
                    seconds_to_wait += 300  # Wait for next candle if we just missed one

                logger.info(f"‚è≥ Next update in {seconds_to_wait}s (at :{next_candle_minute:02d}:05)")
                time.sleep(seconds_to_wait)

        except KeyboardInterrupt:
            logger.info("\n" + "=" * 80)
            logger.info("Bot stopped by user")
            self._print_final_stats()

    def _update_cycle(self):
        """Single update cycle (every 5 minutes)"""
        try:
            # Get market data
            df = self._get_market_data()
            if df is None or len(df) < Phase4TestnetConfig.LOOKBACK_CANDLES:
                logger.warning("Insufficient market data")
                return

            # Calculate features
            df = calculate_features(df)
            df = self.adv_features.calculate_all_features(df)

            # Handle NaN values (from Support/Resistance lookback warmup)
            rows_before = len(df)

            # Identify NaN columns before handling (for informative logging)
            nan_counts = df.isna().sum()
            nan_columns = nan_counts[nan_counts > 0]

            df = df.ffill()
            df = df.dropna()
            rows_after = len(df)
            rows_lost = rows_before - rows_after

            # Expected loss from S/R lookback (50 candles)
            expected_loss = 50  # lookback_sr parameter

            if rows_lost <= expected_loss + 10:  # Normal range (+10 tolerance)
                logger.info(f"‚úÖ Data ready: {rows_after} rows (warmup removed {rows_lost} rows)")
                logger.debug(f"   Expected warmup loss: ~{expected_loss} rows (S/R lookback)")
                if len(nan_columns) > 0:
                    top_nan_cols = nan_columns.nlargest(3)
                    logger.debug(f"   NaN sources: {', '.join(top_nan_cols.index[:3])} (normal)")
            else:
                logger.warning(f"‚ö†Ô∏è Unexpected data loss: {rows_lost} rows (expected ~{expected_loss})")
                logger.warning(f"   This may indicate a data quality issue")
                if len(nan_columns) > 5:
                    logger.warning(f"   {len(nan_columns)} columns have NaN (check feature calculation)")

            if len(df) < 50:
                logger.error(f"‚ùå Insufficient data after NaN handling ({len(df)} < 50)")
                return

            # Current state
            # ‚úÖ FIX: Use previous COMPLETE candle (BingX API returns forming candle as last row!)
            # Root Cause: API returns incomplete candle at iloc[-1], causing 75-91% lower probabilities
            # Solution: Use iloc[-2] for complete candle that matches backtest methodology
            current_idx = len(df) - 2  # Previous complete candle (not forming candle)
            current_price = df['close'].iloc[current_idx]

            # Get account balance
            current_balance = self._get_account_balance()

            # Initialize Buy & Hold if first run
            if not self.bh_initialized:
                self._initialize_buy_hold(current_price)

            # Classify market regime
            current_regime = self._classify_market_regime(df)
            self.market_regime_history.append({
                "timestamp": datetime.now(),
                "regime": current_regime,
                "price": current_price
            })

            # Log market data to institutional logger
            self.inst_logger.log_market_data(
                price=current_price,
                regime=current_regime,
                volume=float(df['volume'].iloc[-1]) if 'volume' in df.columns else None,
                volatility=float(df['atr_pct'].iloc[-1]) if 'atr_pct' in df.columns else None
            )

            logger.info(f"\n{'=' * 80}")
            logger.info(f"Update: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            logger.info(f"Market Regime: {current_regime}")
            logger.info(f"Current Price: ${current_price:,.2f}")
            logger.info(f"Account Balance: ${current_balance:,.2f} USDT")

            # Check current position status from exchange
            current_position = self._get_current_position()

            # Manage existing position
            if current_position is not None:
                # Check for orphaned position (no tracking in trades list)
                has_open_trade = any(t.get('status') == 'OPEN' for t in self.trades)

                if not has_open_trade:
                    logger.warning("‚ö†Ô∏è ORPHANED POSITION DETECTED!")
                    logger.warning(f"   Position: {current_position['position_side']} {current_position['position_amt']:.4f} BTC @ ${current_position['entry_price']:,.2f}")
                    logger.warning(f"   Unrealized P&L: ${current_position['unrealized_pnl']:+,.2f}")
                    logger.warning(f"   Trades in state: {len(self.trades)} (0 OPEN)")
                    logger.warning(f"   Possible causes: Bot crash, manual trade, or state file corruption")
                    logger.warning(f"   Creating trade record with Max Holding time trigger...")

                    # Create trade record with entry_time set to 4 hours ago
                    # This will trigger Max Holding exit immediately
                    orphaned_entry_time = datetime.now() - timedelta(hours=Phase4TestnetConfig.MAX_HOLDING_HOURS)

                    orphaned_trade = {
                        'entry_time': orphaned_entry_time,
                        'order_id': 'ORPHANED',
                        'side': current_position['position_side'],  # ‚úÖ Include position side
                        'entry_price': current_position['entry_price'],
                        'quantity': current_position['position_amt'],
                        'position_size_pct': 0.50,  # Assume 50%
                        'position_value': current_position['entry_price'] * current_position['position_amt'],
                        'regime': 'Unknown',
                        'probability': 0.0,
                        'sizing_factors': {},
                        'status': 'OPEN'
                    }
                    self.trades.append(orphaned_trade)
                    logger.info(f"   ‚úÖ Trade record created - will exit via Max Holding")

                self._manage_position(current_price, df, current_idx, current_position)
            else:
                # Look for new entry (if no position)
                self._check_entry(df, current_idx, current_price, current_regime, current_balance)

            # Print stats (with current API price for accurate B&H comparison)
            self._print_stats(current_price=current_price)

            # Save state
            self._save_state()

        except Exception as e:
            logger.error(f"Error in update cycle: {e}")
            import traceback
            traceback.print_exc()
            # Note: log_error signature: (error_type, error_message, stack_trace)
            self.inst_logger.log_error(
                error_type="UPDATE_CYCLE_ERROR",
                error_message=f"Error in update cycle: {str(e)}",
                stack_trace=traceback.format_exc()
            )

    def _get_market_data(self) -> pd.DataFrame:
        """Get market data from BingX Testnet API"""
        try:
            # BingX API supports up to 1440 candles in single request
            klines = self.client.get_klines(
                symbol=Phase4TestnetConfig.SYMBOL,
                interval=Phase4TestnetConfig.TIMEFRAME,
                limit=Phase4TestnetConfig.LOOKBACK_CANDLES
            )

            if not klines:
                logger.error("No klines data received")
                return None

            # Convert to DataFrame
            df = pd.DataFrame(klines)
            df = df.rename(columns={'time': 'timestamp'})
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            df[['open', 'high', 'low', 'close', 'volume']] = \
                df[['open', 'high', 'low', 'close', 'volume']].astype(float)
            df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']]

            # Sort by timestamp (ensure chronological order)
            df = df.sort_values('timestamp').reset_index(drop=True)

            latest_price = df['close'].iloc[-1]
            latest_time = df['timestamp'].iloc[-1]
            logger.info(f"‚úÖ Live data from BingX Testnet API: {len(df)} candles")
            logger.info(f"   Latest: ${latest_price:,.2f} @ {latest_time.strftime('%Y-%m-%d %H:%M')}")

            return df

        except Exception as e:
            logger.error(f"Failed to get market data: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _initialize_buy_hold(self, current_price):
        """Initialize Buy & Hold baseline"""
        self.bh_entry_price = current_price
        self.bh_btc_quantity = self.initial_balance / current_price
        self.bh_initialized = True

        logger.success(f"üìä Buy & Hold Baseline Initialized (for comparison):")
        logger.info(f"   Virtual Buy: {self.bh_btc_quantity:.6f} BTC @ ${current_price:,.2f}")

    def _classify_market_regime(self, df):
        """Classify current market regime"""
        lookback = 20
        if len(df) < lookback:
            return "Unknown"

        recent_data = df.tail(lookback)
        start_price = recent_data['close'].iloc[0]
        end_price = recent_data['close'].iloc[-1]
        price_change_pct = ((end_price / start_price) - 1) * 100

        if price_change_pct > Phase4TestnetConfig.BULL_THRESHOLD:
            return "Bull"
        elif price_change_pct < Phase4TestnetConfig.BEAR_THRESHOLD:
            return "Bear"
        else:
            return "Sideways"

    def _calculate_dynamic_thresholds(self, df, current_idx):
        """
        Calculate adaptive thresholds based on recent market signal rate

        ROOT CAUSE SOLUTION: Fixed thresholds fail when market regime changes.
        This system automatically adjusts thresholds to maintain target trade frequency
        across different volatility regimes.

        Logic:
        - Monitor recent signal rate (last 6 hours = 72 candles)
        - If signal rate is LOW (4.2%) ‚Üí LOWER threshold (easier to enter)
        - If signal rate is HIGH (16%) ‚Üí RAISE threshold (harder to enter)
        - Target: Maintain 42.5 trades/week consistently

        Args:
            df: DataFrame with features
            current_idx: Current candle index

        Returns:
            dict: {'long': adjusted_long_threshold, 'short': adjusted_short_threshold,
                   'signal_rate': recent_signal_rate, 'adjustment': adjustment_factor}
        """
        if not Phase4TestnetConfig.ENABLE_DYNAMIC_THRESHOLD:
            # Dynamic threshold disabled, use base thresholds
            return {
                'long': Phase4TestnetConfig.BASE_LONG_ENTRY_THRESHOLD,
                'short': Phase4TestnetConfig.BASE_SHORT_ENTRY_THRESHOLD,
                'signal_rate': None,
                'adjustment': 0.0,
                'reason': 'disabled'
            }

        # Calculate lookback window (6 hours = 72 candles)
        lookback_candles = int(Phase4TestnetConfig.DYNAMIC_LOOKBACK_HOURS * 12)

        # Check if we have enough data
        if len(df) < lookback_candles or current_idx < lookback_candles:
            logger.warning(f"‚ö†Ô∏è Insufficient data for dynamic threshold (need {lookback_candles} candles)")
            return {
                'long': Phase4TestnetConfig.BASE_LONG_ENTRY_THRESHOLD,
                'short': Phase4TestnetConfig.BASE_SHORT_ENTRY_THRESHOLD,
                'signal_rate': None,
                'adjustment': 0.0,
                'reason': 'insufficient_data'
            }

        # Get recent data (last 6 hours)
        start_idx = max(0, current_idx - lookback_candles)
        recent_df = df.iloc[start_idx:current_idx]

        # Calculate model probabilities for recent period
        recent_features = recent_df[self.feature_columns].values

        # Remove NaN rows
        valid_mask = ~np.isnan(recent_features).any(axis=1)
        recent_features_clean = recent_features[valid_mask]

        if len(recent_features_clean) < 10:  # Need minimum 10 valid candles
            logger.warning(f"‚ö†Ô∏è Too many NaN in recent data ({len(recent_features_clean)} valid candles)")
            return {
                'long': Phase4TestnetConfig.BASE_LONG_ENTRY_THRESHOLD,
                'short': Phase4TestnetConfig.BASE_SHORT_ENTRY_THRESHOLD,
                'signal_rate': None,
                'adjustment': 0.0,
                'reason': 'nan_data'
            }

        # Apply scaling and predict probabilities
        recent_features_scaled = self.long_scaler.transform(recent_features_clean)
        recent_probs_long = self.long_model.predict_proba(recent_features_scaled)[:, 1]

        # Calculate recent signal rate (what % of recent candles would have signaled with base threshold)
        signals_at_base = (recent_probs_long >= Phase4TestnetConfig.BASE_LONG_ENTRY_THRESHOLD).sum()
        recent_signal_rate = signals_at_base / len(recent_probs_long)

        # Calculate adjustment factor
        # If recent rate is low (0.042) ‚Üí adjustment = 0.042/0.101 = 0.42 ‚Üí lower threshold
        # If recent rate is high (0.16) ‚Üí adjustment = 0.16/0.101 = 1.58 ‚Üí raise threshold
        expected_rate = Phase4TestnetConfig.EXPECTED_SIGNAL_RATE
        adjustment_ratio = recent_signal_rate / expected_rate if expected_rate > 0 else 1.0

        # Convert to threshold adjustment
        # adjustment_ratio < 1.0 ‚Üí lower threshold (market is quiet)
        # adjustment_ratio > 1.0 ‚Üí raise threshold (market is active)
        threshold_delta = (1.0 - adjustment_ratio) * Phase4TestnetConfig.THRESHOLD_ADJUSTMENT_FACTOR

        # Apply adjustment to base thresholds
        adjusted_long = Phase4TestnetConfig.BASE_LONG_ENTRY_THRESHOLD - threshold_delta
        adjusted_short = Phase4TestnetConfig.BASE_SHORT_ENTRY_THRESHOLD - threshold_delta

        # Clip to allowed range
        adjusted_long = np.clip(adjusted_long, Phase4TestnetConfig.MIN_THRESHOLD, Phase4TestnetConfig.MAX_THRESHOLD)
        adjusted_short = np.clip(adjusted_short, Phase4TestnetConfig.MIN_THRESHOLD, Phase4TestnetConfig.MAX_THRESHOLD)

        return {
            'long': adjusted_long,
            'short': adjusted_short,
            'signal_rate': recent_signal_rate,
            'adjustment': threshold_delta,
            'reason': 'regime_adaptive'
        }

    def _check_entry(self, df, idx, current_price, regime, current_balance):
        """Check for entry signal and execute order (DUAL MODEL: LONG + SHORT + DYNAMIC THRESHOLDS)"""
        # Calculate dynamic thresholds based on recent market regime
        dynamic_thresholds = self._calculate_dynamic_thresholds(df, idx)
        threshold_long = dynamic_thresholds['long']
        threshold_short = dynamic_thresholds['short']

        # Log dynamic threshold adjustment
        if dynamic_thresholds['reason'] == 'regime_adaptive':
            signal_rate = dynamic_thresholds['signal_rate']
            adjustment = dynamic_thresholds['adjustment']
            logger.info(f"üéØ Dynamic Threshold System:")
            logger.info(f"  Recent Signal Rate: {signal_rate*100:.1f}% (expected: {Phase4TestnetConfig.EXPECTED_SIGNAL_RATE*100:.1f}%)")
            logger.info(f"  Threshold Adjustment: {adjustment:+.3f}")
            logger.info(f"  LONG Threshold: {threshold_long:.3f} (base: {Phase4TestnetConfig.BASE_LONG_ENTRY_THRESHOLD:.2f})")
            logger.info(f"  SHORT Threshold: {threshold_short:.3f} (base: {Phase4TestnetConfig.BASE_SHORT_ENTRY_THRESHOLD:.2f})")

        # Get features
        features = df[self.feature_columns].iloc[idx:idx+1].values

        if np.isnan(features).any():
            logger.warning("NaN in features, skipping entry check")
            return

        # Apply MinMaxScaler normalization before prediction
        features_long_scaled = self.long_scaler.transform(features)
        features_short_scaled = self.short_scaler.transform(features)

        # Predict with DUAL MODELS (with normalized features)
        prob_long = self.long_model.predict_proba(features_long_scaled)[0][1]   # LONG model
        prob_short = self.short_model.predict_proba(features_short_scaled)[0][1]  # SHORT model

        # Log model predictions to institutional logger (2025-10-15: Dynamic thresholds)
        self.inst_logger.log_model_prediction(
            model_name="LONG_Entry_Model",
            prediction="BUY" if prob_long >= threshold_long else "HOLD",
            confidence=prob_long
        )
        self.inst_logger.log_model_prediction(
            model_name="SHORT_Entry_Model",
            prediction="SELL" if prob_short >= threshold_short else "HOLD",
            confidence=prob_short
        )

        logger.info(f"Signal Check (Dual Model - Dynamic Thresholds 2025-10-15):")
        logger.info(f"  LONG Model Prob: {prob_long:.3f} (dynamic threshold: {threshold_long:.2f})")
        logger.info(f"  SHORT Model Prob: {prob_short:.3f} (dynamic threshold: {threshold_short:.2f})")

        # Determine signal direction (independent models with DYNAMIC thresholds)
        signal_direction = None
        signal_probability = None

        if prob_long >= threshold_long:
            signal_direction = "LONG"
            signal_probability = prob_long
        elif prob_short >= threshold_short:
            signal_direction = "SHORT"
            signal_probability = prob_short

        # Log signal data (for all signals, even if below threshold)
        signal_data = {
            'timestamp': datetime.now(),
            'price': current_price,
            'position_status': 'NONE',
            'position_pnl_pct': None,
            'position_probability': None,
            'current_signal_prob': prob_long,  # Backward compatibility (LONG prob)
            'current_signal_prob_long': prob_long,
            'current_signal_prob_short': prob_short,
            'signal_direction': signal_direction,  # "LONG", "SHORT", or None
            'signal_strength_delta': None,
            'missed_opportunity': False,
            'hours_held': 0,
            'dynamic_threshold_long': threshold_long,  # Track dynamic threshold
            'dynamic_threshold_short': threshold_short,
            'signal_rate_6h': dynamic_thresholds['signal_rate']  # Track recent signal rate
        }
        self.signal_log.append(signal_data)

        # Log signal regardless of threshold (for analysis)
        # Note: log_signal signature: (signal_type, direction, probability, price, features)
        self.inst_logger.log_signal(
            signal_type="ENTRY",
            direction=signal_direction if signal_direction else "NONE",
            probability=signal_probability if signal_probability else max(prob_long, prob_short),
            price=current_price,
            features={
                "long_prob": prob_long,
                "short_prob": prob_short,
                "regime": regime,
                "long_threshold": threshold_long,  # Dynamic threshold
                "short_threshold": threshold_short,  # Dynamic threshold
                "signal_rate_6h": dynamic_thresholds.get('signal_rate'),
                "threshold_reason": dynamic_thresholds.get('reason')
            }
        )

        # Check threshold (different for LONG vs SHORT, DYNAMIC)
        if signal_direction is None:
            logger.info(f"  Should Enter: False (LONG {prob_long:.3f} < {threshold_long:.2f}, SHORT {prob_short:.3f} < {threshold_short:.2f})")
            return

        logger.info(f"  Should Enter: True ({signal_direction} signal = {signal_probability:.3f})")

        # Calculate volatility
        current_volatility = df['atr_pct'].iloc[idx] if 'atr_pct' in df.columns else 0.01
        avg_volatility = df['atr_pct'].iloc[max(0, idx-50):idx].mean() if 'atr_pct' in df.columns else 0.01

        # Calculate dynamic position size
        sizing_result = self.position_sizer.calculate_position_size(
            capital=current_balance,
            signal_strength=signal_probability,
            current_volatility=current_volatility,
            avg_volatility=avg_volatility,
            market_regime=regime,
            recent_trades=self.trades[-10:] if len(self.trades) > 0 else [],
            leverage=Phase4TestnetConfig.LEVERAGE
        )

        position_value = sizing_result['position_value']
        quantity = position_value / current_price

        # Determine order side based on signal direction
        order_side = "BUY" if signal_direction == "LONG" else "SELL"

        # Execute REAL order on Testnet!
        try:
            logger.warning(f"‚ö° EXECUTING REAL ORDER on Testnet!")
            logger.info(f"   Order Type: MARKET {order_side}")
            logger.info(f"   Direction: {signal_direction}")
            logger.info(f"   Quantity: {quantity:.4f} BTC")
            logger.info(f"   Est. Value: ${position_value:,.2f}")

            order_result = self.client.create_order(
                symbol=Phase4TestnetConfig.SYMBOL,
                side=order_side,  # BUY for LONG, SELL for SHORT
                position_side="BOTH",  # Fixed: One-way mode requires "BOTH"
                order_type="MARKET",
                quantity=quantity
            )

            logger.success(f"‚úÖ ORDER EXECUTED!")
            logger.info(f"   Order ID: {order_result.get('orderId')}")
            logger.info(f"   Status: {order_result.get('status')}")
            logger.info(f"   Direction: {signal_direction}")
            logger.info(f"   Position Size: {sizing_result['position_size_pct']*100:.1f}%")
            logger.info(f"   Market Regime: {regime}")
            logger.info(f"   XGBoost Prob: {signal_probability:.3f}")

            # Get actual fill price from exchange (may differ from current_price due to slippage)
            actual_fill_price = order_result.get('average') or order_result.get('price') or current_price

            # Log slippage if significant
            slippage = actual_fill_price - current_price
            if abs(slippage) > 1.0:  # More than $1 slippage
                logger.warning(f"   Slippage: ${slippage:+.2f} (Market: ${current_price:,.2f} ‚Üí Fill: ${actual_fill_price:,.2f})")

            # Record entry with ACTUAL fill price
            trade_record = {
                'entry_time': datetime.now(),
                'order_id': order_result.get('orderId'),
                'side': signal_direction,  # Store LONG or SHORT
                'entry_price': actual_fill_price,  # ‚úÖ ACTUAL fill price (not approximate!)
                'quantity': quantity,
                'position_size_pct': sizing_result['position_size_pct'],
                'position_value': position_value,
                'regime': regime,
                'probability': signal_probability,
                'sizing_factors': sizing_result['factors'],
                'status': 'OPEN'
            }
            self.trades.append(trade_record)

            # Log trade entry to institutional logger (audit trail)
            self.inst_logger.log_trade_entry(
                order_id=str(order_result.get('orderId')),
                side=signal_direction,
                quantity=quantity,
                price=actual_fill_price,
                position_size_pct=sizing_result['position_size_pct'],
                signal_probability=signal_probability,
                regime=regime,
                leverage=float(Phase4TestnetConfig.LEVERAGE)
            )

            logger.info(f"   Entry Price: ${actual_fill_price:,.2f} (filled)")

        except BingXInsufficientBalanceError as e:
            logger.error("‚ùå Insufficient balance for order!")
            # Note: log_error signature: (error_type, error_message, stack_trace)
            import traceback
            self.inst_logger.log_error(
                error_type="INSUFFICIENT_BALANCE",
                error_message=f"Insufficient balance for {order_side} order: {quantity:.4f} BTC @ ${current_price:,.2f}",
                stack_trace=traceback.format_exc()
            )
        except BingXOrderError as e:
            logger.error(f"‚ùå Order failed: {e.message}")
            import traceback
            self.inst_logger.log_error(
                error_type="ORDER_EXECUTION_ERROR",
                error_message=f"Order execution failed ({order_side} {quantity:.4f} BTC): {str(e)}",
                stack_trace=traceback.format_exc()
            )
        except Exception as e:
            logger.error(f"‚ùå Unexpected error: {e}")
            import traceback
            self.inst_logger.log_error(
                error_type="UNEXPECTED_ORDER_ERROR",
                error_message=f"Unexpected error during {order_side} order execution: {str(e)}",
                stack_trace=traceback.format_exc()
            )

    def _manage_position(self, current_price, df, current_idx, position):
        """Manage existing position and check exit conditions"""
        entry_price = position['entry_price']
        quantity = position['position_amt']
        unrealized_pnl = position['unrealized_pnl']

        # Find position side from trades
        position_side = None
        entry_time = None
        position_probability = None
        for trade in reversed(self.trades):
            if trade.get('status') == 'OPEN':
                position_side = trade.get('side', 'LONG')  # Default to LONG for backward compatibility
                entry_time = trade.get('entry_time')
                position_probability = trade.get('probability')
                break

        # Calculate P&L (different formula for LONG vs SHORT)
        if position_side == "SHORT":
            # SHORT: profit when price goes down
            pnl_pct = (entry_price - current_price) / entry_price
        else:
            # LONG: profit when price goes up
            pnl_pct = (current_price - entry_price) / entry_price

        pnl_usd = pnl_pct * (entry_price * quantity)

        # Calculate holding time
        if entry_time:
            hours_held = (datetime.now() - entry_time).total_seconds() / 3600
        else:
            hours_held = 0

        # Check current signal strength (to track missed opportunities)
        features = df[self.feature_columns].iloc[current_idx:current_idx+1].values
        current_signal_prob = None
        signal_strength_delta = None
        missed_opportunity = False

        if not np.isnan(features).any():
            # Use LONG model for signal tracking (backward compatibility)
            current_signal_prob = self.long_model.predict_proba(features)[0][1]
            if position_probability:
                signal_strength_delta = current_signal_prob - position_probability
                # Flag as missed opportunity if current signal is much stronger
                if current_signal_prob >= 0.75 and signal_strength_delta >= 0.10:
                    missed_opportunity = True

        # Log signal data for analysis
        signal_data = {
            'timestamp': datetime.now(),
            'price': current_price,
            'position_status': 'OPEN',
            'position_pnl_pct': pnl_pct,
            'position_probability': position_probability,
            'current_signal_prob': current_signal_prob,  # Backward compatibility
            'current_signal_prob_long': None,  # Not calculated in position management
            'current_signal_prob_short': None,
            'signal_direction': position_side,  # Current position direction
            'signal_strength_delta': signal_strength_delta,
            'missed_opportunity': missed_opportunity,
            'hours_held': hours_held
        }
        self.signal_log.append(signal_data)

        # Calculate current exposure and risk metrics
        current_balance = self._get_account_balance(use_cache=True)
        current_exposure = (entry_price * quantity) / current_balance
        position_value = entry_price * quantity * Phase4TestnetConfig.LEVERAGE

        # Log risk metrics to institutional logger
        self.inst_logger.log_risk_metrics(
            current_exposure=current_exposure,
            var_95=None,  # Can be calculated from historical returns if needed
            var_99=None,
            leverage=float(Phase4TestnetConfig.LEVERAGE),
            max_position_pct=Phase4TestnetConfig.MAX_POSITION_PCT
        )

        logger.info(f"Position: {position_side} {quantity:.4f} BTC @ ${entry_price:,.2f}")
        logger.info(f"P&L: {pnl_pct * 100:+.2f}% (${pnl_usd:+,.2f})")
        logger.info(f"Unrealized PnL (Exchange): ${unrealized_pnl:+,.2f}")
        logger.info(f"Holding: {hours_held:.1f} hours")

        if current_signal_prob is not None:
            # Format entry probability safely
            entry_prob_str = f"{position_probability:.3f}" if position_probability is not None else "N/A"
            logger.info(f"Current Signal: {current_signal_prob:.3f} (Entry: {entry_prob_str})")
            if missed_opportunity:
                logger.warning(f"‚ö†Ô∏è MISSED OPPORTUNITY: Current signal {current_signal_prob:.3f} is +{signal_strength_delta:.3f} stronger!")

        # ============================================================================
        # EXIT DECISION: Dual ML Exit Models (LONG/SHORT specialized)
        # ============================================================================
        exit_reason = None

        # Get current technical features (already fetched above)
        if not np.isnan(features).any():
            # Calculate 8 position-specific features (matching training data)

            # 1. time_held: Normalized by 12 candles (1 hour)
            time_held_normalized = hours_held / 1.0

            # 2. current_pnl_pct: Current P&L percentage (without leverage)
            current_pnl_pct = pnl_pct

            # 3. pnl_peak: Highest P&L since entry
            # Find peak from trade history
            pnl_peak = current_pnl_pct
            for trade in reversed(self.trades):
                if trade.get('status') == 'OPEN':
                    # Calculate historical peak (we don't track this yet, so use current as baseline)
                    pnl_peak = max(pnl_peak, current_pnl_pct)
                    break

            # 4. pnl_trough: Lowest P&L since entry
            pnl_trough = min(current_pnl_pct, 0.0)  # At least 0 (entry point)

            # 5. pnl_from_peak: Distance from peak
            pnl_from_peak = current_pnl_pct - pnl_peak

            # 6. volatility_since_entry: Price volatility since entry
            # Calculate returns since entry
            if len(df) >= 12:  # At least 1 hour of data
                recent_returns = df['close'].iloc[-12:].pct_change().dropna()
                volatility_since_entry = recent_returns.std() if len(recent_returns) > 0 else 0.01
            else:
                volatility_since_entry = 0.01

            # 7. volume_change: Volume change since entry
            entry_volume = df['volume'].iloc[max(0, len(df) - int(hours_held * 12) - 1)]
            current_volume = df['volume'].iloc[-1]
            volume_change = (current_volume - entry_volume) / entry_volume if entry_volume > 0 else 0.0

            # 8. momentum_shift: Recent price momentum
            if len(df) >= 6:  # At least 30 minutes
                recent_returns = df['close'].iloc[-6:].pct_change().dropna()
                momentum_shift = recent_returns.mean() if len(recent_returns) > 0 else 0.0
            else:
                momentum_shift = 0.0

            # Position features array (8 features)
            position_features = np.array([
                time_held_normalized,
                current_pnl_pct,
                pnl_peak,
                pnl_trough,
                pnl_from_peak,
                volatility_since_entry,
                volume_change,
                momentum_shift
            ])

            # Get base technical features (36 from exit model, not 37 from entry model)
            # Exit model was trained with 36 base features
            exit_base_features = [f for f in self.exit_feature_columns if f not in [
                'time_held', 'current_pnl_pct', 'pnl_peak', 'pnl_trough',
                'pnl_from_peak', 'volatility_since_entry', 'volume_change', 'momentum_shift'
            ]]

            # Extract base features from current data
            base_features_values = df[exit_base_features].iloc[current_idx].values

            # Combined features for Exit Model (36 base + 8 position = 44 total)
            exit_features = np.concatenate([base_features_values, position_features]).reshape(1, -1)

            # Select appropriate exit model and scaler based on position direction
            exit_model = self.long_exit_model if position_side == "LONG" else self.short_exit_model
            exit_scaler = self.long_exit_scaler if position_side == "LONG" else self.short_exit_scaler

            # Apply MinMaxScaler normalization before prediction
            exit_features_scaled = exit_scaler.transform(exit_features)

            # Get exit signal from trained Exit Model (with normalized features)
            exit_prob = exit_model.predict_proba(exit_features_scaled)[0][1]

            logger.info(f"Exit Model Signal ({position_side}): {exit_prob:.3f} (threshold: {Phase4TestnetConfig.EXIT_THRESHOLD:.2f})")
            logger.debug(f"  Position Features: time={time_held_normalized:.2f}, pnl={current_pnl_pct*100:.2f}%, peak={pnl_peak*100:.2f}%, from_peak={pnl_from_peak*100:.2f}%")

            # Exit if probability exceeds threshold
            if exit_prob >= Phase4TestnetConfig.EXIT_THRESHOLD:
                exit_reason = f"ML Exit ({position_side} model, prob={exit_prob:.3f})"

        else:
            logger.warning("‚ö†Ô∏è NaN in features, skipping Exit Model (position held)")

        # Safety exits (override ML Exit Model if critical conditions)
        # These act as safety nets for extreme situations where ML exit fails
        # Much more conservative than backtest hard stops (-1.5% SL, +3.5% TP)
        # to allow ML model room to operate optimally
        if pnl_pct <= -0.05:  # -5% emergency stop (very rare with leverage 4x)
            exit_reason = f"Emergency Stop Loss ({pnl_pct*100:.2f}%)"
            logger.error(f"üö® EMERGENCY EXIT: {exit_reason}")
            # Log compliance event for emergency stop
            self.inst_logger.log_compliance_event(
                event_type="EMERGENCY_STOP_LOSS",
                description=f"Emergency stop loss triggered at {pnl_pct*100:.2f}% loss",
                severity="HIGH",
                action_taken="Position closed immediately"
            )
        elif hours_held >= 8:  # 8 hours maximum (3.4x longer than expected 2.36h)
            exit_reason = f"Emergency Max Holding ({hours_held:.1f}h)"
            logger.warning(f"‚ö†Ô∏è EMERGENCY EXIT: {exit_reason}")
            # Log compliance event for max holding violation
            self.inst_logger.log_compliance_event(
                event_type="MAX_HOLDING_EXCEEDED",
                description=f"Position held for {hours_held:.1f}h exceeds 8h limit",
                severity="MEDIUM",
                action_taken="Position closed via emergency exit"
            )

        if exit_reason:
            self._exit_position(current_price, exit_reason, position)

    def _exit_position(self, exit_price, reason, position):
        """Exit position by executing REAL close order"""
        try:
            position_side = position.get('position_side', 'LONG')  # Get actual position side

            logger.warning(f"‚ö° CLOSING POSITION on Testnet!")
            logger.info(f"   Reason: {reason}")
            logger.info(f"   Side: {position_side}")
            logger.info(f"   Quantity: {position['position_amt']:.4f} BTC")

            close_result = self.client.close_position(
                symbol=Phase4TestnetConfig.SYMBOL,
                position_side=position_side,  # Use actual position side (LONG or SHORT)
                quantity=position['position_amt']
            )

            # Validate that close actually succeeded
            # CCXT returns 'id' at top level, not 'orderId'
            order_id = close_result.get('id') or close_result.get('orderId')
            if not close_result or not order_id:
                logger.error(f"‚ùå POSITION CLOSE FAILED!")
                logger.error(f"   API returned: {close_result}")
                logger.error(f"   Trade status remains OPEN")
                return  # Don't mark as closed if API failed

            # Close succeeded - verify position no longer exists
            logger.success(f"‚úÖ POSITION CLOSED!")
            logger.info(f"   Close Order ID: {order_id}")
            logger.info(f"   Exit Price: ${exit_price:,.2f}")

            # Update trade record
            for trade in reversed(self.trades):
                if trade.get('status') == 'OPEN':
                    trade['status'] = 'CLOSED'
                    trade['exit_time'] = datetime.now()
                    trade['exit_price'] = exit_price
                    trade['exit_reason'] = reason
                    trade['close_order_id'] = order_id  # Already extracted above

                    # Calculate final P&L (different for LONG vs SHORT)
                    entry_price = trade['entry_price']
                    quantity = trade['quantity']
                    trade_side = trade.get('side', 'LONG')

                    if trade_side == "SHORT":
                        # SHORT: profit when price goes down
                        pnl_pct = (entry_price - exit_price) / entry_price
                    else:
                        # LONG: profit when price goes up
                        pnl_pct = (exit_price - entry_price) / entry_price

                    pnl_usd = pnl_pct * (entry_price * quantity)

                    # Transaction costs
                    entry_cost = entry_price * quantity * Phase4TestnetConfig.TRANSACTION_COST
                    exit_cost = exit_price * quantity * Phase4TestnetConfig.TRANSACTION_COST
                    total_cost = entry_cost + exit_cost
                    net_pnl = pnl_usd - total_cost

                    trade['pnl_pct'] = pnl_pct
                    trade['pnl_usd_gross'] = pnl_usd
                    trade['transaction_cost'] = total_cost
                    trade['pnl_usd_net'] = net_pnl

                    # Calculate holding time in hours
                    holding_time_hours = (trade['exit_time'] - trade['entry_time']).total_seconds() / 3600

                    # Log trade exit to institutional logger (audit trail with P&L)
                    self.inst_logger.log_trade_exit(
                        order_id=str(order_id),
                        side=trade_side,
                        quantity=quantity,
                        entry_price=entry_price,
                        exit_price=exit_price,
                        pnl_usd=net_pnl,
                        pnl_pct=pnl_pct,
                        holding_time_hours=holding_time_hours,
                        exit_reason=reason,
                        transaction_costs=total_cost
                    )

                    logger.info(f"   Gross P&L: {pnl_pct * 100:+.2f}% (${pnl_usd:+,.2f})")
                    logger.info(f"   Transaction Cost: ${total_cost:.2f}")
                    logger.info(f"   Net P&L: ${net_pnl:+,.2f}")
                    break

        except Exception as e:
            logger.error(f"‚ùå Failed to close position: {e}")
            logger.error(f"   Trade status remains OPEN (exception occurred)")
            # Note: log_error signature: (error_type, error_message, stack_trace)
            import traceback
            self.inst_logger.log_error(
                error_type="POSITION_CLOSE_ERROR",
                error_message=f"Failed to close {position.get('position_side')} position ({position.get('position_amt')} BTC): {str(e)}",
                stack_trace=traceback.format_exc()
            )

    def _print_stats(self, current_price=None):
        """Print current performance statistics"""
        closed_trades = [t for t in self.trades if t.get('status') == 'CLOSED']

        if len(closed_trades) == 0:
            logger.info(f"\nüìä No completed trades yet")
            return

        df_trades = pd.DataFrame(closed_trades)

        # Basic stats
        total_trades = len(df_trades)
        winning_trades = len(df_trades[df_trades['pnl_usd_net'] > 0])
        win_rate = (winning_trades / total_trades) * 100

        # Returns
        total_net_pnl = df_trades['pnl_usd_net'].sum()
        current_balance = self._get_account_balance(use_cache=True)
        total_return_pct = ((current_balance - self.initial_balance) / self.initial_balance) * 100

        # Average position size
        avg_position_size = df_trades['position_size_pct'].mean() * 100

        # Buy & Hold comparison
        if self.bh_initialized and current_price is not None:
            bh_value = self.bh_btc_quantity * current_price
            bh_return_pct = ((bh_value - self.initial_balance) / self.initial_balance) * 100
            vs_bh = total_return_pct - bh_return_pct
        else:
            bh_return_pct = 0.0
            vs_bh = 0.0

        # Time-based metrics
        days_running = (datetime.now() - self.session_start).total_seconds() / 86400
        trades_per_week = (total_trades / days_running) * 7 if days_running > 0 else 0

        # Calculate Sharpe and Sortino ratios (if enough trades)
        sharpe_ratio = None
        sortino_ratio = None
        if len(df_trades) >= 10:
            returns = df_trades['pnl_pct'].values
            sharpe_ratio = (returns.mean() / returns.std()) * np.sqrt(252) if returns.std() > 0 else None
            downside_returns = returns[returns < 0]
            if len(downside_returns) > 0 and downside_returns.std() > 0:
                sortino_ratio = (returns.mean() / downside_returns.std()) * np.sqrt(252)

        # Calculate max drawdown
        equity_curve = (df_trades['pnl_usd_net'].cumsum() + self.initial_balance).values
        max_drawdown = None
        if len(equity_curve) > 0:
            peak = np.maximum.accumulate(equity_curve)
            drawdown = (equity_curve - peak) / peak
            max_drawdown = drawdown.min() if len(drawdown) > 0 else None

        # Calculate profit factor
        winning_pnl = df_trades[df_trades['pnl_usd_net'] > 0]['pnl_usd_net'].sum()
        losing_pnl = abs(df_trades[df_trades['pnl_usd_net'] < 0]['pnl_usd_net'].sum())
        profit_factor = winning_pnl / losing_pnl if losing_pnl > 0 else None

        # Log performance metrics to institutional logger
        self.inst_logger.log_performance_metrics(
            current_capital=current_balance,
            total_trades=total_trades,
            win_rate=win_rate,
            sharpe_ratio=sharpe_ratio,
            sortino_ratio=sortino_ratio,
            max_drawdown=max_drawdown,
            profit_factor=profit_factor
        )

        logger.info(f"\n{'=' * 80}")
        logger.info("üìä PHASE 4 TESTNET TRADING PERFORMANCE")
        logger.info(f"{'=' * 80}")
        logger.info(f"Session Duration: {days_running:.1f} days")
        logger.info(f"")
        # LONG vs SHORT Distribution (2025-10-15: Track threshold optimization effectiveness)
        long_trades = df_trades[df_trades['side'] == 'LONG']
        short_trades = df_trades[df_trades['side'] == 'SHORT']
        long_count = len(long_trades)
        short_count = len(short_trades)
        long_pct = (long_count / total_trades * 100) if total_trades > 0 else 0
        short_pct = (short_count / total_trades * 100) if total_trades > 0 else 0

        # Win rates by direction
        long_win_rate = (len(long_trades[long_trades['pnl_usd_net'] > 0]) / long_count * 100) if long_count > 0 else 0
        short_win_rate = (len(short_trades[short_trades['pnl_usd_net'] > 0]) / short_count * 100) if short_count > 0 else 0

        # Trades per week by direction
        long_trades_per_week = (long_count / days_running) * 7 if days_running > 0 else 0
        short_trades_per_week = (short_count / days_running) * 7 if days_running > 0 else 0

        logger.info(f"Trading Performance:")
        logger.info(f"  Total Trades: {total_trades}")
        logger.info(f"  Winning: {winning_trades} ({win_rate:.1f}%) {'‚úÖ' if win_rate >= Phase4TestnetConfig.TARGET_WIN_RATE else '‚ö†Ô∏è'}")
        logger.info(f"  Trades/Week: {trades_per_week:.1f}")
        logger.info(f"  Avg Position: {avg_position_size:.1f}%")
        logger.info(f"")
        logger.info(f"Trade Distribution (Symmetric Model 2025-10-15):")
        logger.info(f"  LONG: {long_count} trades ({long_pct:.1f}%) - {long_trades_per_week:.1f}/week - Win: {long_win_rate:.1f}%")
        logger.info(f"  SHORT: {short_count} trades ({short_pct:.1f}%) - {short_trades_per_week:.1f}/week - Win: {short_win_rate:.1f}%")
        logger.info(f"  Expected: 79% LONG (17.7/week, 28% prec) / 21% SHORT (4.6/week, 79% prec!)")
        logger.info(f"")
        logger.info(f"Returns:")
        logger.info(f"  Total Net P&L: ${total_net_pnl:+,.2f}")
        logger.info(f"  Total Return: {total_return_pct:+.2f}%")
        logger.info(f"  Current Balance: ${current_balance:,.2f} USDT")
        logger.info(f"  Initial Balance: ${self.initial_balance:,.2f} USDT")
        logger.info(f"")
        logger.info(f"vs Buy & Hold:")
        logger.info(f"  B&H Return: {bh_return_pct:+.2f}%")
        logger.info(f"  Strategy Return: {total_return_pct:+.2f}%")
        logger.info(f"  Difference: {vs_bh:+.2f}% {'‚úÖ' if vs_bh > 0 else '‚ö†Ô∏è'}")
        logger.info(f"{'=' * 80}")

    def _print_final_stats(self):
        """Print final statistics on exit"""
        self._print_stats()

        # Generate and log session summary
        session_duration = (datetime.now() - self.session_start).total_seconds() / 3600
        closed_trades = [t for t in self.trades if t.get('status') == 'CLOSED']
        open_trades = [t for t in self.trades if t.get('status') == 'OPEN']

        if len(closed_trades) > 0:
            df_trades = pd.DataFrame(closed_trades)
            total_pnl = df_trades['pnl_usd_net'].sum()
            win_rate = (len(df_trades[df_trades['pnl_usd_net'] > 0]) / len(df_trades)) * 100
        else:
            total_pnl = 0.0
            win_rate = 0.0

        # Generate session summary
        summary = self.inst_logger.generate_session_summary(
            session_duration_hours=session_duration,
            final_capital=self._get_account_balance(use_cache=True),
            total_trades=len(closed_trades),
            open_positions=len(open_trades)
        )

        logger.info("\n" + "=" * 80)
        logger.info("üìä SESSION SUMMARY (Institutional Format)")
        logger.info("=" * 80)
        logger.info(f"Duration: {session_duration:.1f} hours")
        logger.info(f"Total Trades: {len(closed_trades)} closed, {len(open_trades)} open")
        logger.info(f"Win Rate: {win_rate:.1f}%")
        logger.info(f"Total P&L: ${total_pnl:+,.2f}")
        logger.info(f"Log files:")
        logger.info(f"  - JSON: {summary.get('json_log_path', 'N/A')}")
        logger.info(f"  - Text: {summary.get('text_log_path', 'N/A')}")
        logger.info(f"  - Audit: {summary.get('audit_log_path', 'N/A')}")
        logger.info("=" * 80)

        # Save trades to CSV
        if len(closed_trades) > 0:
            df_trades = pd.DataFrame(closed_trades)
            output_file = RESULTS_DIR / f"phase4_testnet_trading_trades_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            df_trades.to_csv(output_file, index=False)
            logger.success(f"\n‚úÖ Trades saved to: {output_file}")

        # Save signal log for opportunity cost analysis
        if len(self.signal_log) > 0:
            df_signals = pd.DataFrame(self.signal_log)
            signal_file = RESULTS_DIR / f"phase4_testnet_signal_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            df_signals.to_csv(signal_file, index=False)
            logger.success(f"‚úÖ Signal log saved to: {signal_file}")

            # Print opportunity cost analysis
            missed_opps = df_signals[df_signals['missed_opportunity'] == True]
            if len(missed_opps) > 0:
                logger.warning(f"\n‚ö†Ô∏è MISSED OPPORTUNITIES DETECTED:")
                logger.warning(f"   Count: {len(missed_opps)}")
                logger.warning(f"   Avg Signal Delta: +{missed_opps['signal_strength_delta'].mean():.3f}")
                logger.warning(f"   Max Signal Delta: +{missed_opps['signal_strength_delta'].max():.3f}")
                logger.warning(f"   See {signal_file} for details")

    def _save_state(self):
        """Save bot state with trades persistence"""
        # Serialize trades list (convert datetime to ISO format)
        serializable_trades = []
        for trade in self.trades:
            trade_copy = trade.copy()
            # Convert datetime objects to ISO format strings
            if 'entry_time' in trade_copy and isinstance(trade_copy['entry_time'], datetime):
                trade_copy['entry_time'] = trade_copy['entry_time'].isoformat()
            if 'exit_time' in trade_copy and isinstance(trade_copy['exit_time'], datetime):
                trade_copy['exit_time'] = trade_copy['exit_time'].isoformat()
            serializable_trades.append(trade_copy)

        state = {
            "initial_balance": self.initial_balance,
            "current_balance": self._get_account_balance(use_cache=True),
            "trades": serializable_trades,  # Now persists full trades list
            "trades_count": len(self.trades),
            "closed_trades": len([t for t in self.trades if t.get('status') == 'CLOSED']),
            "bh_btc_quantity": self.bh_btc_quantity,
            "bh_entry_price": self.bh_entry_price,
            "session_start": self.session_start.isoformat(),
            "timestamp": datetime.now().isoformat()
        }

        state_file = RESULTS_DIR / "phase4_testnet_trading_state.json"
        with open(state_file, 'w') as f:
            json.dump(state, f, indent=2, default=str)

# ============================================================================
# Main
# ============================================================================

def main():
    """Main entry point"""
    logger.info("=" * 80)
    logger.info("Phase 4 Dynamic Position Sizing TESTNET Trading Bot")
    logger.info("‚ö†Ô∏è WARNING: This bot executes REAL orders on BingX Testnet!")
    logger.info("=" * 80)

    # Check API credentials
    if not Phase4TestnetConfig.API_KEY or not Phase4TestnetConfig.API_SECRET:
        logger.error("BingX API credentials not found!")
        logger.error("Set BINGX_API_KEY and BINGX_API_SECRET environment variables")
        return

    # Check model exists
    model_path = MODELS_DIR / "xgboost_v4_phase4_advanced_lookahead3_thresh0.pkl"
    if not model_path.exists():
        logger.error(f"XGBoost Phase 4 model not found: {model_path}")
        return

    # Use Singleton pattern to prevent duplicate instances
    with BotSingleton():
        # Initialize and run bot
        try:
            bot = Phase4DynamicTestnetTradingBot()
            bot.run()
        except KeyboardInterrupt:
            logger.info("\n" + "=" * 80)
            logger.info("Bot stopped by user (Ctrl+C)")
            logger.info("=" * 80)
        except Exception as e:
            logger.error(f"Bot failed with exception: {e}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    main()
