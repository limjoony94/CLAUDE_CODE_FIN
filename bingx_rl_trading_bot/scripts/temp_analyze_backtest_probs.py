#!/usr/bin/env python
"""
ë°±í…ŒìŠ¤íŠ¸ ê¸°ê°„ì˜ ì‹¤ì œ LONG/SHORT í™•ë¥  ë¶„í¬ ë¶„ì„

ë°±í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©í•œ ë°ì´í„°ì™€ ê°™ì€ ëª¨ë¸ë¡œ í™•ë¥  ê³„ì‚°í•˜ì—¬
ì‹¤ì œ ë°±í…ŒìŠ¤íŠ¸ ë•Œì˜ í™•ë¥  ë¶„í¬ë¥¼ í™•ì¸
"""
import pandas as pd
import numpy as np
import pickle
from pathlib import Path
import sys

PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from scripts.experiments.calculate_all_features import calculate_all_features

DATA_DIR = PROJECT_ROOT / "data" / "historical"
MODELS_DIR = PROJECT_ROOT / "models"

print("=" * 80)
print("ë°±í…ŒìŠ¤íŠ¸ ê¸°ê°„ LONG/SHORT í™•ë¥  ë¶„í¬ ë¶„ì„")
print("=" * 80)
print()

# Load models (SAME AS PRODUCTION BOT - 10/18 UPGRADED)
print("1ï¸âƒ£ ëª¨ë¸ ë¡œë”© ì¤‘...")
print("   âš ï¸  í”„ë¡œë•ì…˜ ë´‡ê³¼ ë™ì¼í•œ ëª¨ë¸ ì‚¬ìš© (10/18 ì—…ê·¸ë ˆì´ë“œ)")

with open(MODELS_DIR / "xgboost_long_trade_outcome_full_20251018_233146.pkl", 'rb') as f:
    long_model = pickle.load(f)

with open(MODELS_DIR / "xgboost_long_trade_outcome_full_20251018_233146_scaler.pkl", 'rb') as f:
    long_scaler = pickle.load(f)

with open(MODELS_DIR / "xgboost_long_trade_outcome_full_20251018_233146_features.txt", 'r') as f:
    long_feature_columns = [line.strip() for line in f.readlines()]

with open(MODELS_DIR / "xgboost_short_trade_outcome_full_20251018_233146.pkl", 'rb') as f:
    short_model = pickle.load(f)

with open(MODELS_DIR / "xgboost_short_trade_outcome_full_20251018_233146_scaler.pkl", 'rb') as f:
    short_scaler = pickle.load(f)

with open(MODELS_DIR / "xgboost_short_trade_outcome_full_20251018_233146_features.txt", 'r') as f:
    short_feature_columns = [line.strip() for line in f.readlines()]

print("   âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
print()

# Load historical data (same as backtest)
print("2ï¸âƒ£ ë°±í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”© ì¤‘...")
csv_files = sorted(DATA_DIR.glob("btcusdt_5m_*.csv"))
print(f"   ë°œê²¬ëœ íŒŒì¼: {len(csv_files)}ê°œ")

dfs = []
for csv_file in csv_files:
    df = pd.read_csv(csv_file)
    dfs.append(df)

df_full = pd.concat(dfs, ignore_index=True)
df_full['timestamp'] = pd.to_datetime(df_full['timestamp'])
df_full = df_full.sort_values('timestamp').reset_index(drop=True)

print(f"   ë°ì´í„° ê¸°ê°„: {df_full['timestamp'].min()} ~ {df_full['timestamp'].max()}")
print(f"   ì´ ìº”ë“¤ ìˆ˜: {len(df_full):,}")
print()

# Calculate features
print("3ï¸âƒ£ Feature ê³„ì‚° ì¤‘...")
df_features = calculate_all_features(df_full.copy())
print(f"   âœ… Feature ê³„ì‚° ì™„ë£Œ ({len(df_features):,} rows)")
print()

# Drop rows with NaN
df_clean = df_features.dropna(subset=long_feature_columns + short_feature_columns)
print(f"   ìœ íš¨í•œ í–‰: {len(df_clean):,} (NaN ì œê±° í›„)")
print()

# Calculate probabilities
print("4ï¸âƒ£ LONG/SHORT í™•ë¥  ê³„ì‚° ì¤‘...")

# Prepare features
X_long = df_clean[long_feature_columns].values
X_short = df_clean[short_feature_columns].values

# Scale features
X_long_scaled = long_scaler.transform(X_long)
X_short_scaled = short_scaler.transform(X_short)

# Predict probabilities
long_probs = long_model.predict_proba(X_long_scaled)[:, 1]
short_probs = short_model.predict_proba(X_short_scaled)[:, 1]

print("   âœ… í™•ë¥  ê³„ì‚° ì™„ë£Œ")
print()

# Analyze distribution
print("=" * 80)
print("ğŸ“Š ë°±í…ŒìŠ¤íŠ¸ ê¸°ê°„ í™•ë¥  ë¶„í¬ (ì „ì²´ ë°ì´í„°)")
print("=" * 80)
print()

print("LONG Probability í†µê³„:")
print(f"  í‰ê· : {long_probs.mean():.4f}")
print(f"  ì¤‘ì•™ê°’: {np.median(long_probs):.4f}")
print(f"  í‘œì¤€í¸ì°¨: {long_probs.std():.4f}")
print(f"  ìµœì†Œ: {long_probs.min():.4f}")
print(f"  ìµœëŒ€: {long_probs.max():.4f}")
print()

print("LONG Probability ë¶„í¬:")
print(f"  < 0.3: {(long_probs < 0.3).sum():,} ({(long_probs < 0.3).sum()/len(long_probs)*100:.1f}%)")
print(f"  0.3-0.5: {((long_probs >= 0.3) & (long_probs < 0.5)).sum():,} ({((long_probs >= 0.3) & (long_probs < 0.5)).sum()/len(long_probs)*100:.1f}%)")
print(f"  0.5-0.65: {((long_probs >= 0.5) & (long_probs < 0.65)).sum():,} ({((long_probs >= 0.5) & (long_probs < 0.65)).sum()/len(long_probs)*100:.1f}%)")
print(f"  0.65-0.8: {((long_probs >= 0.65) & (long_probs < 0.8)).sum():,} ({((long_probs >= 0.65) & (long_probs < 0.8)).sum()/len(long_probs)*100:.1f}%)")
print(f"  >= 0.8: {(long_probs >= 0.8).sum():,} ({(long_probs >= 0.8).sum()/len(long_probs)*100:.1f}%)")
print()

print("SHORT Probability í†µê³„:")
print(f"  í‰ê· : {short_probs.mean():.4f}")
print(f"  ì¤‘ì•™ê°’: {np.median(short_probs):.4f}")
print(f"  í‘œì¤€í¸ì°¨: {short_probs.std():.4f}")
print(f"  ìµœì†Œ: {short_probs.min():.4f}")
print(f"  ìµœëŒ€: {short_probs.max():.4f}")
print()

print("SHORT Probability ë¶„í¬:")
print(f"  < 0.1: {(short_probs < 0.1).sum():,} ({(short_probs < 0.1).sum()/len(short_probs)*100:.1f}%)")
print(f"  0.1-0.3: {((short_probs >= 0.1) & (short_probs < 0.3)).sum():,} ({((short_probs >= 0.1) & (short_probs < 0.3)).sum()/len(short_probs)*100:.1f}%)")
print(f"  0.3-0.5: {((short_probs >= 0.3) & (short_probs < 0.5)).sum():,} ({((short_probs >= 0.3) & (short_probs < 0.5)).sum()/len(short_probs)*100:.1f}%)")
print(f"  0.5-0.7: {((short_probs >= 0.5) & (short_probs < 0.7)).sum():,} ({((short_probs >= 0.5) & (short_probs < 0.7)).sum()/len(short_probs)*100:.1f}%)")
print(f"  >= 0.7: {(short_probs >= 0.7).sum():,} ({(short_probs >= 0.7).sum()/len(short_probs)*100:.1f}%)")
print()

print("=" * 80)
print("ğŸ¯ Threshold ì´ˆê³¼ ë¹„ìœ¨ (ì‹¤ì œ ì§„ì… ì¡°ê±´)")
print("=" * 80)
print(f"LONG >= 0.65 (ì§„ì…): {(long_probs >= 0.65).sum():,} ({(long_probs >= 0.65).sum()/len(long_probs)*100:.1f}%)")
print(f"SHORT >= 0.70 (í›„ë³´): {(short_probs >= 0.70).sum():,} ({(short_probs >= 0.70).sum()/len(short_probs)*100:.1f}%)")
print()
print(f"ì´ ìƒ˜í”Œ ìˆ˜: {len(long_probs):,}")
print()

# Compare with live data
print("=" * 80)
print("ğŸ“Š ì‹¤ì œ ìš´ì˜ vs ë°±í…ŒìŠ¤íŠ¸ ë¹„êµ")
print("=" * 80)
print()

# Live data (from previous analysis - 10/19)
live_long_mean = 0.8147
live_long_median = 0.8595
live_long_above_065 = 88.9
live_short_mean = 0.0030
live_short_median = 0.0012

backtest_long_mean = long_probs.mean()
backtest_long_median = np.median(long_probs)
backtest_long_above_065 = (long_probs >= 0.65).sum() / len(long_probs) * 100
backtest_short_mean = short_probs.mean()
backtest_short_median = np.median(short_probs)

print("LONG í™•ë¥ :")
print(f"  í‰ê· :    ì‹¤ì œ {live_long_mean:.4f} | ë°±í…ŒìŠ¤íŠ¸ {backtest_long_mean:.4f} | ì°¨ì´: {abs(live_long_mean - backtest_long_mean):.4f}")
print(f"  ì¤‘ì•™ê°’:  ì‹¤ì œ {live_long_median:.4f} | ë°±í…ŒìŠ¤íŠ¸ {backtest_long_median:.4f} | ì°¨ì´: {abs(live_long_median - backtest_long_median):.4f}")
print(f"  >= 0.65: ì‹¤ì œ {live_long_above_065:.1f}% | ë°±í…ŒìŠ¤íŠ¸ {backtest_long_above_065:.1f}% | ì°¨ì´: {abs(live_long_above_065 - backtest_long_above_065):.1f}%p")
print()

print("SHORT í™•ë¥ :")
print(f"  í‰ê· :    ì‹¤ì œ {live_short_mean:.4f} | ë°±í…ŒìŠ¤íŠ¸ {backtest_short_mean:.4f} | ì°¨ì´: {abs(live_short_mean - backtest_short_mean):.4f}")
print(f"  ì¤‘ì•™ê°’:  ì‹¤ì œ {live_short_median:.4f} | ë°±í…ŒìŠ¤íŠ¸ {backtest_short_median:.4f} | ì°¨ì´: {abs(live_short_median - backtest_short_median):.4f}")
print()

# Conclusion
print("=" * 80)
print("ğŸ” ë¶„ì„ ê²°ë¡ ")
print("=" * 80)
print()

if abs(live_long_mean - backtest_long_mean) < 0.2:
    print("âœ… ì‹¤ì œ ìš´ì˜ê³¼ ë°±í…ŒìŠ¤íŠ¸ì˜ LONG í™•ë¥  í‰ê· ì´ ìœ ì‚¬í•©ë‹ˆë‹¤.")
else:
    print("âš ï¸  ì‹¤ì œ ìš´ì˜ê³¼ ë°±í…ŒìŠ¤íŠ¸ì˜ LONG í™•ë¥  í‰ê· ì— ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤.")
    print(f"   ì°¨ì´: {abs(live_long_mean - backtest_long_mean):.4f}")

print()

if abs(live_long_above_065 - backtest_long_above_065) < 20:
    print("âœ… ì§„ì… ì¡°ê±´ ë§Œì¡± ë¹„ìœ¨ì´ ë°±í…ŒìŠ¤íŠ¸ì™€ ìœ ì‚¬í•©ë‹ˆë‹¤.")
else:
    print("âš ï¸  ì§„ì… ì¡°ê±´ ë§Œì¡± ë¹„ìœ¨ì´ ë°±í…ŒìŠ¤íŠ¸ì™€ ë‹¤ë¦…ë‹ˆë‹¤.")
    print(f"   ì°¨ì´: {abs(live_long_above_065 - backtest_long_above_065):.1f}%p")

print()
print("ğŸ’¡ ì°¸ê³ :")
print("   - ë°±í…ŒìŠ¤íŠ¸ëŠ” ì „ì²´ ê¸°ê°„ í‰ê· ì´ë¯€ë¡œ ë‹¨ê¸° ë³€ë™ê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
print("   - ì‹œì¥ ìƒí™©ì— ë”°ë¼ í™•ë¥  ë¶„í¬ê°€ ë³€í•˜ëŠ” ê²ƒì€ ì •ìƒì…ë‹ˆë‹¤")
print("   - ì¤‘ìš”í•œ ê²ƒì€ ëª¨ë¸ì˜ ì˜ˆì¸¡ë ¥(ìŠ¹ë¥ )ì´ì§€ í™•ë¥  ë¶„í¬ ìì²´ê°€ ì•„ë‹™ë‹ˆë‹¤")
print()
