#!/usr/bin/env python
"""
ìµœê·¼ ìº”ë“¤ ë°ì´í„°ë¥¼ ì§ì ‘ ë¡œë”©í•˜ì—¬ LONG/SHORT í™•ë¥  ê³„ì‚°

ëª©ì : ë°±í…ŒìŠ¤íŠ¸ vs ì‹¤ì œ ìš´ì˜ ì°¨ì´ê°€ ë°ì´í„° ë¬¸ì œì¸ì§€ ì‹œì¥ ë³€í™”ì¸ì§€ í™•ì¸
"""
import pandas as pd
import numpy as np
import pickle
from pathlib import Path
import sys
from datetime import datetime, timedelta

PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from scripts.experiments.calculate_all_features import calculate_all_features

MODELS_DIR = PROJECT_ROOT / "models"

print("=" * 80)
print("ìµœê·¼ ìº”ë“¤ ë°ì´í„° ì§ì ‘ ë¶„ì„ (ë‚ ì§œë³„)")
print("=" * 80)
print()

# Load models
print("1ï¸âƒ£ ëª¨ë¸ ë¡œë”© ì¤‘...")
with open(MODELS_DIR / "xgboost_v4_phase4_advanced_lookahead3_thresh0.pkl", 'rb') as f:
    long_model = pickle.load(f)

with open(MODELS_DIR / "xgboost_v4_phase4_advanced_lookahead3_thresh0_scaler.pkl", 'rb') as f:
    long_scaler = pickle.load(f)

with open(MODELS_DIR / "xgboost_v4_phase4_advanced_lookahead3_thresh0_features.txt", 'r') as f:
    long_feature_columns = [line.strip() for line in f.readlines()]

with open(MODELS_DIR / "xgboost_short_redesigned_20251016_233322.pkl", 'rb') as f:
    short_model = pickle.load(f)

with open(MODELS_DIR / "xgboost_short_redesigned_20251016_233322_scaler.pkl", 'rb') as f:
    short_scaler = pickle.load(f)

with open(MODELS_DIR / "xgboost_short_redesigned_20251016_233322_features.txt", 'r') as f:
    short_feature_columns = [line.strip() for line in f.readlines()]

print("   âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
print()

# Load historical data
print("2ï¸âƒ£ íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ë¡œë”© ì¤‘...")
DATA_DIR = PROJECT_ROOT / "data" / "historical"
csv_files = sorted(DATA_DIR.glob("btcusdt_5m_*.csv"))

dfs = []
for csv_file in csv_files:
    df = pd.read_csv(csv_file)
    dfs.append(df)

df_full = pd.concat(dfs, ignore_index=True)
df_full['timestamp'] = pd.to_datetime(df_full['timestamp'])
df_full = df_full.sort_values('timestamp').reset_index(drop=True)

print(f"   ì „ì²´ ë°ì´í„°: {df_full['timestamp'].min()} ~ {df_full['timestamp'].max()}")
print(f"   ì´ ìº”ë“¤ ìˆ˜: {len(df_full):,}")
print()

# Filter to recent dates
print("3ï¸âƒ£ ë‚ ì§œë³„ ë°ì´í„° í•„í„°ë§ ë° ë¶„ì„...")
print()

# Dates to analyze
dates_to_check = [
    ('2025-10-15', '2025-10-16'),  # 10/15
    ('2025-10-16', '2025-10-17'),  # 10/16
    ('2025-10-17', '2025-10-18'),  # 10/17
    ('2025-10-18', '2025-10-19'),  # 10/18
    ('2025-10-19', '2025-10-20'),  # 10/19 (ì˜¤ëŠ˜)
]

results_summary = []

for start_date, end_date in dates_to_check:
    print("=" * 80)
    print(f"ğŸ“… {start_date} ë¶„ì„")
    print("=" * 80)

    # Filter data for this date
    mask = (df_full['timestamp'] >= start_date) & (df_full['timestamp'] < end_date)
    df_date = df_full[mask].copy()

    if len(df_date) == 0:
        print(f"   âš ï¸ {start_date} ë°ì´í„° ì—†ìŒ")
        print()
        continue

    print(f"   ìº”ë“¤ ìˆ˜: {len(df_date)}")
    print(f"   ì‹œê°„ ë²”ìœ„: {df_date['timestamp'].min()} ~ {df_date['timestamp'].max()}")
    print(f"   ê°€ê²© ë²”ìœ„: ${df_date['close'].min():,.1f} ~ ${df_date['close'].max():,.1f}")
    print()

    # Need enough history for features (minimum 200 candles)
    # Get data from 2 days before to have enough history
    start_with_history = pd.to_datetime(start_date) - timedelta(days=2)
    mask_with_history = (df_full['timestamp'] >= start_with_history) & (df_full['timestamp'] < end_date)
    df_with_history = df_full[mask_with_history].copy()

    print(f"   Feature ê³„ì‚°ìš© ë°ì´í„°: {len(df_with_history)} ìº”ë“¤ (history í¬í•¨)")

    # Calculate features
    df_features = calculate_all_features(df_with_history.copy())

    # Filter back to target date
    df_features_date = df_features[
        (df_features['timestamp'] >= start_date) &
        (df_features['timestamp'] < end_date)
    ].copy()

    # Drop NaN
    df_clean = df_features_date.dropna(subset=long_feature_columns + short_feature_columns)

    if len(df_clean) == 0:
        print(f"   âš ï¸ ìœ íš¨í•œ ë°ì´í„° ì—†ìŒ (NaN ì œê±° í›„)")
        print()
        continue

    print(f"   ìœ íš¨í•œ ìº”ë“¤: {len(df_clean)} (NaN ì œê±° í›„)")
    print()

    # Calculate probabilities
    X_long = df_clean[long_feature_columns].values
    X_short = df_clean[short_feature_columns].values

    X_long_scaled = long_scaler.transform(X_long)
    X_short_scaled = short_scaler.transform(X_short)

    long_probs = long_model.predict_proba(X_long_scaled)[:, 1]
    short_probs = short_model.predict_proba(X_short_scaled)[:, 1]

    # Statistics
    print("   LONG í™•ë¥  í†µê³„:")
    print(f"     í‰ê· : {long_probs.mean():.4f}")
    print(f"     ì¤‘ì•™ê°’: {np.median(long_probs):.4f}")
    print(f"     ìµœì†Œ: {long_probs.min():.4f}")
    print(f"     ìµœëŒ€: {long_probs.max():.4f}")
    print(f"     >= 0.65: {(long_probs >= 0.65).sum()} ({(long_probs >= 0.65).sum()/len(long_probs)*100:.1f}%)")
    print()

    print("   SHORT í™•ë¥  í†µê³„:")
    print(f"     í‰ê· : {short_probs.mean():.4f}")
    print(f"     ì¤‘ì•™ê°’: {np.median(short_probs):.4f}")
    print(f"     >= 0.70: {(short_probs >= 0.70).sum()} ({(short_probs >= 0.70).sum()/len(short_probs)*100:.1f}%)")
    print()

    # Price statistics
    price_change = ((df_clean['close'].iloc[-1] / df_clean['close'].iloc[0]) - 1) * 100
    print(f"   ê°€ê²© ë³€í™”: ${df_clean['close'].iloc[0]:,.1f} â†’ ${df_clean['close'].iloc[-1]:,.1f} ({price_change:+.2f}%)")
    print()

    # Save summary
    results_summary.append({
        'date': start_date,
        'candles': len(df_clean),
        'long_mean': long_probs.mean(),
        'long_median': np.median(long_probs),
        'long_above_065': (long_probs >= 0.65).sum() / len(long_probs) * 100,
        'short_mean': short_probs.mean(),
        'short_above_070': (short_probs >= 0.70).sum() / len(short_probs) * 100,
        'price_start': df_clean['close'].iloc[0],
        'price_end': df_clean['close'].iloc[-1],
        'price_change_pct': price_change
    })

# Summary table
print("=" * 80)
print("ğŸ“Š ë‚ ì§œë³„ ìš”ì•½")
print("=" * 80)
print()

df_summary = pd.DataFrame(results_summary)
print("ë‚ ì§œ         | ìº”ë“¤ìˆ˜ | LONGí‰ê·  | LONG>=0.65 | SHORTí‰ê·  | ê°€ê²©ë³€í™”")
print("-" * 80)
for _, row in df_summary.iterrows():
    print(f"{row['date']} | {row['candles']:4d}ê°œ | {row['long_mean']:6.4f}  | {row['long_above_065']:6.1f}%  | {row['short_mean']:7.4f} | {row['price_change_pct']:+6.2f}%")

print()
print("=" * 80)
print("ğŸ” ì¶”ì„¸ ë¶„ì„")
print("=" * 80)
print()

# Trend analysis
if len(df_summary) >= 2:
    first_day = df_summary.iloc[0]
    last_day = df_summary.iloc[-1]

    print(f"LONG í™•ë¥  ë³€í™”:")
    print(f"  {first_day['date']}: {first_day['long_mean']:.4f} (í‰ê· )")
    print(f"  {last_day['date']}: {last_day['long_mean']:.4f} (í‰ê· )")
    print(f"  ë³€í™”: {last_day['long_mean'] - first_day['long_mean']:+.4f}")
    print()

    print(f"ì§„ì… ê¸°íšŒ (LONG >= 0.65) ë³€í™”:")
    print(f"  {first_day['date']}: {first_day['long_above_065']:.1f}%")
    print(f"  {last_day['date']}: {last_day['long_above_065']:.1f}%")
    print(f"  ë³€í™”: {last_day['long_above_065'] - first_day['long_above_065']:+.1f}%p")
    print()

print("=" * 80)
print("ğŸ’¡ ê²°ë¡ ")
print("=" * 80)
print()

if len(df_summary) > 0:
    # Check if recent day is unusually high
    recent_long_mean = df_summary.iloc[-1]['long_mean']

    if recent_long_mean > 0.5:
        print("âš ï¸  ìµœê·¼ LONG í™•ë¥ ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤!")
        print("   - ë°±í…ŒìŠ¤íŠ¸ í‰ê·  (0.0972)ë³´ë‹¤ í›¨ì”¬ ë†’ìŒ")
        print("   - ì‹œì¥ì´ ê°•í•œ bullish ìƒíƒœì´ê±°ë‚˜")
        print("   - ëª¨ë¸ì´ ê³¼ë„í•˜ê²Œ ë°˜ì‘í•˜ê³  ìˆì„ ê°€ëŠ¥ì„±")
        print()
        print("   ê¶Œì¥: ì‹¤ì œ ì§„ì… í›„ ìŠ¹ë¥  í™•ì¸ í•„ìš”")
    elif recent_long_mean < 0.1:
        print("âœ… ìµœê·¼ LONG í™•ë¥ ì´ ë°±í…ŒìŠ¤íŠ¸ì™€ ìœ ì‚¬í•©ë‹ˆë‹¤.")
        print("   - ë°±í…ŒìŠ¤íŠ¸ í‰ê· : 0.0972")
        print("   - ì •ìƒì ì¸ ë²”ìœ„")
    else:
        print("ğŸ“Š ìµœê·¼ LONG í™•ë¥ ì´ ë°±í…ŒìŠ¤íŠ¸ë³´ë‹¤ ë‹¤ì†Œ ë†’ìŠµë‹ˆë‹¤.")
        print("   - ì‹œì¥ ìƒí™© ë³€í™”ë¡œ ë³´ì„")
        print("   - ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ í•„ìš”")

print()
